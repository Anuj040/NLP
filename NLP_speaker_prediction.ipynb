{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_speaker_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pfH9HnJgwTWK",
        "iLKRiDpAPrvu",
        "7DczwG0MDXUd",
        "rg4iWhakQKDp",
        "4N1M55dBUVBY"
      ],
      "authorship_tag": "ABX9TyOwJG94T5qYYhDaDiQZqvvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anuj040/NLP/blob/master/NLP_speaker_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3U-aDOiD1mx"
      },
      "source": [
        "The content of this notebook has been motivated from https://www.kaggle.com/johnwdata/eda-and-nlp-on-democratic-debate-transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfH9HnJgwTWK"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NralFxI1yFuQ"
      },
      "source": [
        "* Before starting, download the dataset from https://www.kaggle.com/brandenciranni/democratic-debate-transcripts-2020\n",
        "* You can then upload the dataset to the colab, as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGnUcTnaQrE1"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Now4kmPIwdMT"
      },
      "source": [
        "# Upload the data file\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bazMRyYtAfEL"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Prepare dataframe and check for null values\n",
        "df = pd.read_csv(\"/content/debate_transcripts_v3_2020-02-26.csv\", encoding=\"cp1252\", usecols= [\"speaker\", \"speech\"])\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy_wJuxqBV-1"
      },
      "source": [
        "# For the specified columns, there are no null values\n",
        "# so, we can proceed as such\n",
        "# check the samples available for each speaker\n",
        "# information might be useful in case of imbalanced dataset \n",
        "value_counts = df.speaker.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svTnptGdB-1Q"
      },
      "source": [
        "# Remove speakers with very few sample\n",
        "# Identify the speakers with speech count less than 100\n",
        "to_remove = value_counts[value_counts <= 100].index\n",
        "\n",
        "# Keep rows where the city column is not in to_remove\n",
        "df = df[~df.speaker.isin(to_remove)]\n",
        "# Get the list of unique speakers\n",
        "speakers = list(df.speaker.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLKRiDpAPrvu"
      },
      "source": [
        "## One hot encode the speaker labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PvBWd-2PPHW"
      },
      "source": [
        "# Get one hot encoding of columns B\n",
        "one_hot = pd.get_dummies(df[\"speaker\"])\n",
        "# Join the encoded df\n",
        "df = df.join(one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DczwG0MDXUd"
      },
      "source": [
        "## Clean the speech, removing filler/stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AR6eWhPDMvk",
        "outputId": "46a60056-a956-4822-975c-8e3749bb7f4d"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# add a column for the speech with stop words and punctuation removed\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "add_words = {\"its\", \"would\", \"us\", \"then\", \"so\", \"it\", \"thats\", \"going\", \"also\", \"crosstalk\"}\n",
        "stop_words =  stop_words.union(add_words)\n",
        "df[\"speech_cleaned\"] = df[\"speech\"].apply(lambda x: \" \".join([re.sub(r'[^\\w\\d]','', item.lower()) for item in x.split() if re.sub(r'[^\\w\\d]','', item.lower()) not in stop_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruGP_0hDHZx1"
      },
      "source": [
        "### Clean up the stems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eV8cUmrHJj7"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# Create stemmer\n",
        "stemmer = PorterStemmer()\n",
        "# Stem cleaned up speech in the debate data\n",
        "df[\"speech_cleaned\"] = df[\"speech_cleaned\"].apply(lambda x: \" \".join([stemmer.stem(item) for item in x.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XWOSmOwGg0B"
      },
      "source": [
        "## Prepare the train/test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN2DKtTIFwhs"
      },
      "source": [
        "train=df.sample(frac=0.8,random_state=101) #random state is a seed value\n",
        "test=df.drop(train.index).sample(frac=1.0,random_state=45)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "speech_tokenize = Tokenizer()\n",
        "# set max sequence length\n",
        "max_len = 150\n",
        "X_train = pad_sequences(speech_tokenize.texts_to_sequences(train[\"speech_cleaned\"]), maxlen=max_len, padding=\"post\")\n",
        "Y_train = train[speakers]\n",
        "\n",
        "X_test = pad_sequences(speech_tokenize.texts_to_sequences(test[\"speech_cleaned\"]), maxlen=max_len, padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jREplOPsDZb"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19YYaRBDIxo8"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, BatchNormalization, Activation, Bidirectional, LSTM\n",
        "# build a model for speech analysis\n",
        "speaker_model = Sequential([\n",
        "    Embedding(len(speech_tokenize.word_index) + 1, 200),\n",
        "    Bidirectional(LSTM(32, return_sequences=True)),\n",
        "    Bidirectional(LSTM(16)),\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(16),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(len(speakers\n",
        "    ), activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg4iWhakQKDp"
      },
      "source": [
        "## Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK8dLC5OQJzd",
        "outputId": "65f5f3fb-4e41-4290-fb53-56b50e01c52f"
      },
      "source": [
        "speaker_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "# Test if the model is working as expected\n",
        "# speech = df[\"speech_cleaned\"][0]\n",
        "# predict = speaker_model.predict(pad_sequences(speech_tokenize.texts_to_sequences([speech]), maxlen=max_len, padding=\"post\"))\n",
        "# predict.shape\n",
        "\n",
        "speaker_model.fit(X_train, Y_train, validation_split=.1, epochs=100, verbose=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "86/86 - 9s - loss: 2.7933 - acc: 0.0988 - val_loss: 2.7542 - val_acc: 0.1176\n",
            "Epoch 2/100\n",
            "86/86 - 2s - loss: 2.7201 - acc: 0.1112 - val_loss: 2.6987 - val_acc: 0.0915\n",
            "Epoch 3/100\n",
            "86/86 - 2s - loss: 2.6998 - acc: 0.1152 - val_loss: 3.1526 - val_acc: 0.0915\n",
            "Epoch 4/100\n",
            "86/86 - 2s - loss: 2.6959 - acc: 0.1148 - val_loss: 4.8746 - val_acc: 0.0915\n",
            "Epoch 5/100\n",
            "86/86 - 2s - loss: 2.6915 - acc: 0.1217 - val_loss: 9.9763 - val_acc: 0.0915\n",
            "Epoch 6/100\n",
            "86/86 - 2s - loss: 2.6911 - acc: 0.1119 - val_loss: 42.4488 - val_acc: 0.0915\n",
            "Epoch 7/100\n",
            "86/86 - 2s - loss: 2.6888 - acc: 0.1137 - val_loss: 54.2911 - val_acc: 0.0915\n",
            "Epoch 8/100\n",
            "86/86 - 2s - loss: 2.6837 - acc: 0.1210 - val_loss: 107.4386 - val_acc: 0.1536\n",
            "Epoch 9/100\n",
            "86/86 - 2s - loss: 2.6829 - acc: 0.1050 - val_loss: 79.4706 - val_acc: 0.1176\n",
            "Epoch 10/100\n",
            "86/86 - 2s - loss: 2.6814 - acc: 0.1159 - val_loss: 98.6460 - val_acc: 0.1536\n",
            "Epoch 11/100\n",
            "86/86 - 2s - loss: 2.6866 - acc: 0.1181 - val_loss: 64.5547 - val_acc: 0.0915\n",
            "Epoch 12/100\n",
            "86/86 - 3s - loss: 2.6817 - acc: 0.1123 - val_loss: 58.8041 - val_acc: 0.0915\n",
            "Epoch 13/100\n",
            "86/86 - 2s - loss: 2.6821 - acc: 0.1177 - val_loss: 72.9414 - val_acc: 0.0915\n",
            "Epoch 14/100\n",
            "86/86 - 2s - loss: 2.6816 - acc: 0.1210 - val_loss: 47.9421 - val_acc: 0.1176\n",
            "Epoch 15/100\n",
            "86/86 - 2s - loss: 2.6802 - acc: 0.1166 - val_loss: 36.6126 - val_acc: 0.1176\n",
            "Epoch 16/100\n",
            "86/86 - 2s - loss: 2.6807 - acc: 0.1152 - val_loss: 35.7622 - val_acc: 0.1176\n",
            "Epoch 17/100\n",
            "86/86 - 2s - loss: 2.6816 - acc: 0.1112 - val_loss: 18.7956 - val_acc: 0.0915\n",
            "Epoch 18/100\n",
            "86/86 - 3s - loss: 2.6808 - acc: 0.1119 - val_loss: 41.6478 - val_acc: 0.0915\n",
            "Epoch 19/100\n",
            "86/86 - 2s - loss: 2.6785 - acc: 0.1156 - val_loss: 15.9412 - val_acc: 0.0915\n",
            "Epoch 20/100\n",
            "86/86 - 2s - loss: 2.6819 - acc: 0.1232 - val_loss: 7.0942 - val_acc: 0.0915\n",
            "Epoch 21/100\n",
            "86/86 - 2s - loss: 2.6808 - acc: 0.1210 - val_loss: 3.8982 - val_acc: 0.0915\n",
            "Epoch 22/100\n",
            "86/86 - 3s - loss: 2.6823 - acc: 0.1177 - val_loss: 2.8834 - val_acc: 0.0915\n",
            "Epoch 23/100\n",
            "86/86 - 2s - loss: 2.6792 - acc: 0.1181 - val_loss: 2.6966 - val_acc: 0.0915\n",
            "Epoch 24/100\n",
            "86/86 - 2s - loss: 2.6817 - acc: 0.1225 - val_loss: 2.6912 - val_acc: 0.0915\n",
            "Epoch 25/100\n",
            "86/86 - 3s - loss: 2.6797 - acc: 0.1239 - val_loss: 2.6937 - val_acc: 0.0915\n",
            "Epoch 26/100\n",
            "86/86 - 3s - loss: 2.6801 - acc: 0.1188 - val_loss: 2.6940 - val_acc: 0.1536\n",
            "Epoch 27/100\n",
            "86/86 - 3s - loss: 2.6787 - acc: 0.1228 - val_loss: 2.6955 - val_acc: 0.1176\n",
            "Epoch 28/100\n",
            "86/86 - 3s - loss: 2.6791 - acc: 0.1119 - val_loss: 2.6935 - val_acc: 0.1176\n",
            "Epoch 29/100\n",
            "86/86 - 2s - loss: 2.6807 - acc: 0.1246 - val_loss: 2.7069 - val_acc: 0.1176\n",
            "Epoch 30/100\n",
            "86/86 - 2s - loss: 2.6789 - acc: 0.1221 - val_loss: 2.7198 - val_acc: 0.1176\n",
            "Epoch 31/100\n",
            "86/86 - 2s - loss: 2.6812 - acc: 0.1217 - val_loss: 2.6919 - val_acc: 0.1176\n",
            "Epoch 32/100\n",
            "86/86 - 2s - loss: 2.6797 - acc: 0.1177 - val_loss: 2.6911 - val_acc: 0.0882\n",
            "Epoch 33/100\n",
            "86/86 - 3s - loss: 2.6793 - acc: 0.1152 - val_loss: 2.7010 - val_acc: 0.0915\n",
            "Epoch 34/100\n",
            "86/86 - 3s - loss: 2.6792 - acc: 0.1206 - val_loss: 2.6861 - val_acc: 0.0915\n",
            "Epoch 35/100\n",
            "86/86 - 2s - loss: 2.6809 - acc: 0.1166 - val_loss: 2.6894 - val_acc: 0.0915\n",
            "Epoch 36/100\n",
            "86/86 - 2s - loss: 2.6808 - acc: 0.1181 - val_loss: 2.6879 - val_acc: 0.0882\n",
            "Epoch 37/100\n",
            "86/86 - 3s - loss: 2.6805 - acc: 0.1214 - val_loss: 2.6807 - val_acc: 0.1176\n",
            "Epoch 38/100\n",
            "86/86 - 3s - loss: 2.6807 - acc: 0.1228 - val_loss: 2.6895 - val_acc: 0.1176\n",
            "Epoch 39/100\n",
            "86/86 - 2s - loss: 2.6818 - acc: 0.1195 - val_loss: 2.6831 - val_acc: 0.1536\n",
            "Epoch 40/100\n",
            "86/86 - 3s - loss: 2.6787 - acc: 0.1188 - val_loss: 2.6911 - val_acc: 0.0915\n",
            "Epoch 41/100\n",
            "86/86 - 3s - loss: 2.6806 - acc: 0.1152 - val_loss: 2.6868 - val_acc: 0.0882\n",
            "Epoch 42/100\n",
            "86/86 - 3s - loss: 2.6812 - acc: 0.1217 - val_loss: 2.6833 - val_acc: 0.1176\n",
            "Epoch 43/100\n",
            "86/86 - 3s - loss: 2.6797 - acc: 0.1246 - val_loss: 2.6882 - val_acc: 0.0915\n",
            "Epoch 44/100\n",
            "86/86 - 3s - loss: 2.6817 - acc: 0.1177 - val_loss: 2.6980 - val_acc: 0.1176\n",
            "Epoch 45/100\n",
            "86/86 - 2s - loss: 2.6791 - acc: 0.1214 - val_loss: 2.7009 - val_acc: 0.0915\n",
            "Epoch 46/100\n",
            "86/86 - 2s - loss: 2.6792 - acc: 0.1188 - val_loss: 2.7029 - val_acc: 0.0915\n",
            "Epoch 47/100\n",
            "86/86 - 2s - loss: 2.6818 - acc: 0.1210 - val_loss: 2.7267 - val_acc: 0.0915\n",
            "Epoch 48/100\n",
            "86/86 - 3s - loss: 2.6815 - acc: 0.1188 - val_loss: 2.7024 - val_acc: 0.0915\n",
            "Epoch 49/100\n",
            "86/86 - 3s - loss: 2.6774 - acc: 0.1239 - val_loss: 2.6891 - val_acc: 0.0915\n",
            "Epoch 50/100\n",
            "86/86 - 2s - loss: 2.6809 - acc: 0.1210 - val_loss: 2.6882 - val_acc: 0.0915\n",
            "Epoch 51/100\n",
            "86/86 - 3s - loss: 2.6788 - acc: 0.1214 - val_loss: 2.6895 - val_acc: 0.0915\n",
            "Epoch 52/100\n",
            "86/86 - 3s - loss: 2.6803 - acc: 0.1195 - val_loss: 2.6873 - val_acc: 0.0915\n",
            "Epoch 53/100\n",
            "86/86 - 3s - loss: 2.6805 - acc: 0.1145 - val_loss: 2.6882 - val_acc: 0.0915\n",
            "Epoch 54/100\n",
            "86/86 - 2s - loss: 2.6797 - acc: 0.1203 - val_loss: 2.6845 - val_acc: 0.0915\n",
            "Epoch 55/100\n",
            "86/86 - 2s - loss: 2.6804 - acc: 0.1174 - val_loss: 2.6850 - val_acc: 0.0915\n",
            "Epoch 56/100\n",
            "86/86 - 2s - loss: 2.6793 - acc: 0.1188 - val_loss: 2.6882 - val_acc: 0.1536\n",
            "Epoch 57/100\n",
            "86/86 - 2s - loss: 2.6800 - acc: 0.1214 - val_loss: 2.6887 - val_acc: 0.0915\n",
            "Epoch 58/100\n",
            "86/86 - 2s - loss: 2.6803 - acc: 0.1199 - val_loss: 2.6995 - val_acc: 0.0915\n",
            "Epoch 59/100\n",
            "86/86 - 2s - loss: 2.6807 - acc: 0.1192 - val_loss: 2.7002 - val_acc: 0.0915\n",
            "Epoch 60/100\n",
            "86/86 - 2s - loss: 2.6807 - acc: 0.1145 - val_loss: 2.6911 - val_acc: 0.1176\n",
            "Epoch 61/100\n",
            "86/86 - 2s - loss: 2.6802 - acc: 0.1221 - val_loss: 2.7160 - val_acc: 0.0915\n",
            "Epoch 62/100\n",
            "86/86 - 2s - loss: 2.6779 - acc: 0.1243 - val_loss: 2.7039 - val_acc: 0.0915\n",
            "Epoch 63/100\n",
            "86/86 - 2s - loss: 2.6806 - acc: 0.1203 - val_loss: 2.6878 - val_acc: 0.0915\n",
            "Epoch 64/100\n",
            "86/86 - 3s - loss: 2.6787 - acc: 0.1199 - val_loss: 2.7009 - val_acc: 0.0915\n",
            "Epoch 65/100\n",
            "86/86 - 3s - loss: 2.6803 - acc: 0.1217 - val_loss: 2.6965 - val_acc: 0.1176\n",
            "Epoch 66/100\n",
            "86/86 - 3s - loss: 2.6789 - acc: 0.1235 - val_loss: 2.6882 - val_acc: 0.0915\n",
            "Epoch 67/100\n",
            "86/86 - 3s - loss: 2.6791 - acc: 0.1203 - val_loss: 2.6919 - val_acc: 0.0882\n",
            "Epoch 68/100\n",
            "86/86 - 2s - loss: 2.6800 - acc: 0.1181 - val_loss: 2.6903 - val_acc: 0.1536\n",
            "Epoch 69/100\n",
            "86/86 - 2s - loss: 2.6794 - acc: 0.1188 - val_loss: 2.7205 - val_acc: 0.1176\n",
            "Epoch 70/100\n",
            "86/86 - 2s - loss: 2.6805 - acc: 0.1228 - val_loss: 2.7214 - val_acc: 0.1176\n",
            "Epoch 71/100\n",
            "86/86 - 2s - loss: 2.6796 - acc: 0.1221 - val_loss: 2.7057 - val_acc: 0.0915\n",
            "Epoch 72/100\n",
            "86/86 - 2s - loss: 2.6800 - acc: 0.1210 - val_loss: 2.7038 - val_acc: 0.0915\n",
            "Epoch 73/100\n",
            "86/86 - 2s - loss: 2.6811 - acc: 0.1195 - val_loss: 2.7000 - val_acc: 0.0915\n",
            "Epoch 74/100\n",
            "86/86 - 2s - loss: 2.6787 - acc: 0.1221 - val_loss: 2.6888 - val_acc: 0.0915\n",
            "Epoch 75/100\n",
            "86/86 - 2s - loss: 2.6783 - acc: 0.1232 - val_loss: 2.6945 - val_acc: 0.0915\n",
            "Epoch 76/100\n",
            "86/86 - 2s - loss: 2.6811 - acc: 0.1217 - val_loss: 2.6892 - val_acc: 0.0882\n",
            "Epoch 77/100\n",
            "86/86 - 3s - loss: 2.6809 - acc: 0.1206 - val_loss: 2.6926 - val_acc: 0.0915\n",
            "Epoch 78/100\n",
            "86/86 - 2s - loss: 2.6796 - acc: 0.1254 - val_loss: 2.7144 - val_acc: 0.0915\n",
            "Epoch 79/100\n",
            "86/86 - 2s - loss: 2.6812 - acc: 0.1228 - val_loss: 2.6997 - val_acc: 0.0915\n",
            "Epoch 80/100\n",
            "86/86 - 2s - loss: 2.6812 - acc: 0.1214 - val_loss: 2.6938 - val_acc: 0.1176\n",
            "Epoch 81/100\n",
            "86/86 - 2s - loss: 2.6794 - acc: 0.1203 - val_loss: 2.6974 - val_acc: 0.0915\n",
            "Epoch 82/100\n",
            "86/86 - 2s - loss: 2.6786 - acc: 0.1210 - val_loss: 2.7104 - val_acc: 0.0915\n",
            "Epoch 83/100\n",
            "86/86 - 2s - loss: 2.6795 - acc: 0.1228 - val_loss: 2.6930 - val_acc: 0.0915\n",
            "Epoch 84/100\n",
            "86/86 - 2s - loss: 2.6794 - acc: 0.1206 - val_loss: 2.7005 - val_acc: 0.0915\n",
            "Epoch 85/100\n",
            "86/86 - 2s - loss: 2.6799 - acc: 0.1210 - val_loss: 2.7010 - val_acc: 0.0915\n",
            "Epoch 86/100\n",
            "86/86 - 2s - loss: 2.6788 - acc: 0.1206 - val_loss: 2.7041 - val_acc: 0.0915\n",
            "Epoch 87/100\n",
            "86/86 - 2s - loss: 2.6815 - acc: 0.1210 - val_loss: 2.6985 - val_acc: 0.0915\n",
            "Epoch 88/100\n",
            "86/86 - 2s - loss: 2.6811 - acc: 0.1243 - val_loss: 2.7115 - val_acc: 0.0915\n",
            "Epoch 89/100\n",
            "86/86 - 2s - loss: 2.6780 - acc: 0.1210 - val_loss: 2.6944 - val_acc: 0.0915\n",
            "Epoch 90/100\n",
            "86/86 - 2s - loss: 2.6780 - acc: 0.1239 - val_loss: 2.6902 - val_acc: 0.0915\n",
            "Epoch 91/100\n",
            "86/86 - 3s - loss: 2.6777 - acc: 0.1243 - val_loss: 2.6988 - val_acc: 0.0915\n",
            "Epoch 92/100\n",
            "86/86 - 3s - loss: 2.6797 - acc: 0.1210 - val_loss: 2.7468 - val_acc: 0.0915\n",
            "Epoch 93/100\n",
            "86/86 - 3s - loss: 2.6782 - acc: 0.1275 - val_loss: 2.7211 - val_acc: 0.0915\n",
            "Epoch 94/100\n",
            "86/86 - 2s - loss: 2.6807 - acc: 0.1210 - val_loss: 2.7111 - val_acc: 0.0915\n",
            "Epoch 95/100\n",
            "86/86 - 2s - loss: 2.6792 - acc: 0.1235 - val_loss: 2.7187 - val_acc: 0.0915\n",
            "Epoch 96/100\n",
            "86/86 - 2s - loss: 2.6799 - acc: 0.1217 - val_loss: 2.7113 - val_acc: 0.0915\n",
            "Epoch 97/100\n",
            "86/86 - 2s - loss: 2.6790 - acc: 0.1221 - val_loss: 2.7320 - val_acc: 0.0915\n",
            "Epoch 98/100\n",
            "86/86 - 2s - loss: 2.6784 - acc: 0.1221 - val_loss: 2.7414 - val_acc: 0.0915\n",
            "Epoch 99/100\n",
            "86/86 - 2s - loss: 2.6784 - acc: 0.1221 - val_loss: 2.7042 - val_acc: 0.1176\n",
            "Epoch 100/100\n",
            "86/86 - 2s - loss: 2.6803 - acc: 0.1174 - val_loss: 2.7236 - val_acc: 0.0915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe072404dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K38Na2KjUDrV"
      },
      "source": [
        "* As can be seen, with this naive approach, the model seems to be not learning much. One possible cause could be the data imbalance. To remedy that, we can try various different approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N1M55dBUVBY"
      },
      "source": [
        "## Experimenting with weighing losses for different labels depending on their occuring frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km5rKWMmZh2E"
      },
      "source": [
        "import numpy as np\n",
        "value_counts = train.speaker.value_counts()\n",
        "# Higher weight to label with lower occurence\n",
        "loss_weights = np.array(value_counts.sum()/value_counts.sort_index())\n",
        "loss_weights = loss_weights/max(loss_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9EGo_WHa2ae"
      },
      "source": [
        "### Recompile the model with additional parameter for loss weights and retrain the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-d9dOt-bCVC",
        "outputId": "946055ef-a04c-4af1-c25b-d136b830e745"
      },
      "source": [
        "speaker_model_2 = Sequential([\n",
        "    Embedding(len(speech_tokenize.word_index) + 1, 64),\n",
        "    Bidirectional(LSTM(32, return_sequences=True)),\n",
        "    Bidirectional(LSTM(16)),\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(16),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(len(speakers\n",
        "    ), activation=\"softmax\")\n",
        "])\n",
        "speaker_model_2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"], loss_weights=loss_weights)\n",
        "speaker_model_2.fit(X_train, Y_train, validation_split=.1, epochs=100, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "86/86 - 7s - loss: 1.6437 - acc: 0.0970 - val_loss: 1.6324 - val_acc: 0.0915\n",
            "Epoch 2/100\n",
            "86/86 - 2s - loss: 1.6241 - acc: 0.1130 - val_loss: 1.6111 - val_acc: 0.1176\n",
            "Epoch 3/100\n",
            "86/86 - 2s - loss: 1.5979 - acc: 0.1188 - val_loss: 1.5863 - val_acc: 0.1176\n",
            "Epoch 4/100\n",
            "86/86 - 2s - loss: 1.5805 - acc: 0.1181 - val_loss: 1.9737 - val_acc: 0.0915\n",
            "Epoch 5/100\n",
            "86/86 - 2s - loss: 1.5755 - acc: 0.1145 - val_loss: 1.9779 - val_acc: 0.1536\n",
            "Epoch 6/100\n",
            "86/86 - 2s - loss: 1.5698 - acc: 0.1221 - val_loss: 5.4193 - val_acc: 0.1536\n",
            "Epoch 7/100\n",
            "86/86 - 2s - loss: 1.5692 - acc: 0.1076 - val_loss: 41.5574 - val_acc: 0.0882\n",
            "Epoch 8/100\n",
            "86/86 - 2s - loss: 1.5686 - acc: 0.1130 - val_loss: 53.8889 - val_acc: 0.1176\n",
            "Epoch 9/100\n",
            "86/86 - 2s - loss: 1.5670 - acc: 0.1145 - val_loss: 56.7676 - val_acc: 0.0915\n",
            "Epoch 10/100\n",
            "86/86 - 2s - loss: 1.5655 - acc: 0.1239 - val_loss: 18.9681 - val_acc: 0.0915\n",
            "Epoch 11/100\n",
            "86/86 - 2s - loss: 1.5644 - acc: 0.1174 - val_loss: 66.6400 - val_acc: 0.0915\n",
            "Epoch 12/100\n",
            "86/86 - 2s - loss: 1.5677 - acc: 0.1192 - val_loss: 83.8453 - val_acc: 0.0915\n",
            "Epoch 13/100\n",
            "86/86 - 2s - loss: 1.5638 - acc: 0.1232 - val_loss: 20.7330 - val_acc: 0.0915\n",
            "Epoch 14/100\n",
            "86/86 - 2s - loss: 1.5678 - acc: 0.1126 - val_loss: 35.5214 - val_acc: 0.0915\n",
            "Epoch 15/100\n",
            "86/86 - 2s - loss: 1.5652 - acc: 0.1203 - val_loss: 52.0429 - val_acc: 0.0915\n",
            "Epoch 16/100\n",
            "86/86 - 2s - loss: 1.5632 - acc: 0.1206 - val_loss: 13.9562 - val_acc: 0.0915\n",
            "Epoch 17/100\n",
            "86/86 - 2s - loss: 1.5631 - acc: 0.1268 - val_loss: 18.1109 - val_acc: 0.1176\n",
            "Epoch 18/100\n",
            "86/86 - 2s - loss: 1.5624 - acc: 0.1265 - val_loss: 16.9730 - val_acc: 0.0915\n",
            "Epoch 19/100\n",
            "86/86 - 2s - loss: 1.5614 - acc: 0.1272 - val_loss: 30.4835 - val_acc: 0.0915\n",
            "Epoch 20/100\n",
            "86/86 - 2s - loss: 1.5625 - acc: 0.1181 - val_loss: 14.2552 - val_acc: 0.0915\n",
            "Epoch 21/100\n",
            "86/86 - 2s - loss: 1.5639 - acc: 0.1126 - val_loss: 11.6905 - val_acc: 0.1176\n",
            "Epoch 22/100\n",
            "86/86 - 2s - loss: 1.5626 - acc: 0.1086 - val_loss: 30.6639 - val_acc: 0.1176\n",
            "Epoch 23/100\n",
            "86/86 - 2s - loss: 1.5632 - acc: 0.1243 - val_loss: 15.4087 - val_acc: 0.0915\n",
            "Epoch 24/100\n",
            "86/86 - 2s - loss: 1.5626 - acc: 0.1210 - val_loss: 6.5150 - val_acc: 0.1176\n",
            "Epoch 25/100\n",
            "86/86 - 2s - loss: 1.5630 - acc: 0.1141 - val_loss: 10.3399 - val_acc: 0.1176\n",
            "Epoch 26/100\n",
            "86/86 - 2s - loss: 1.5630 - acc: 0.1159 - val_loss: 8.3266 - val_acc: 0.0948\n",
            "Epoch 27/100\n",
            "86/86 - 2s - loss: 1.5630 - acc: 0.1192 - val_loss: 8.8683 - val_acc: 0.1176\n",
            "Epoch 28/100\n",
            "86/86 - 2s - loss: 1.5613 - acc: 0.1159 - val_loss: 5.6165 - val_acc: 0.1176\n",
            "Epoch 29/100\n",
            "86/86 - 2s - loss: 1.5632 - acc: 0.1152 - val_loss: 2.8518 - val_acc: 0.1176\n",
            "Epoch 30/100\n",
            "86/86 - 2s - loss: 1.5619 - acc: 0.1203 - val_loss: 1.9131 - val_acc: 0.1176\n",
            "Epoch 31/100\n",
            "86/86 - 2s - loss: 1.5618 - acc: 0.1290 - val_loss: 1.6481 - val_acc: 0.0915\n",
            "Epoch 32/100\n",
            "86/86 - 2s - loss: 1.5615 - acc: 0.1170 - val_loss: 1.5851 - val_acc: 0.0915\n",
            "Epoch 33/100\n",
            "86/86 - 2s - loss: 1.5629 - acc: 0.1185 - val_loss: 1.5712 - val_acc: 0.0915\n",
            "Epoch 34/100\n",
            "86/86 - 2s - loss: 1.5613 - acc: 0.1137 - val_loss: 1.5757 - val_acc: 0.0915\n",
            "Epoch 35/100\n",
            "86/86 - 2s - loss: 1.5617 - acc: 0.1177 - val_loss: 1.5815 - val_acc: 0.0915\n",
            "Epoch 36/100\n",
            "86/86 - 2s - loss: 1.5615 - acc: 0.1177 - val_loss: 1.5728 - val_acc: 0.0915\n",
            "Epoch 37/100\n",
            "86/86 - 2s - loss: 1.5612 - acc: 0.1174 - val_loss: 1.5650 - val_acc: 0.0915\n",
            "Epoch 38/100\n",
            "86/86 - 2s - loss: 1.5621 - acc: 0.1097 - val_loss: 1.5675 - val_acc: 0.1536\n",
            "Epoch 39/100\n",
            "86/86 - 2s - loss: 1.5625 - acc: 0.1217 - val_loss: 1.5918 - val_acc: 0.1536\n",
            "Epoch 40/100\n",
            "86/86 - 2s - loss: 1.5634 - acc: 0.1174 - val_loss: 1.5776 - val_acc: 0.1536\n",
            "Epoch 41/100\n",
            "86/86 - 2s - loss: 1.5626 - acc: 0.1156 - val_loss: 1.5802 - val_acc: 0.0915\n",
            "Epoch 42/100\n",
            "86/86 - 2s - loss: 1.5625 - acc: 0.1203 - val_loss: 1.5739 - val_acc: 0.1176\n",
            "Epoch 43/100\n",
            "86/86 - 2s - loss: 1.5613 - acc: 0.1195 - val_loss: 1.5679 - val_acc: 0.1536\n",
            "Epoch 44/100\n",
            "86/86 - 2s - loss: 1.5632 - acc: 0.1185 - val_loss: 1.5700 - val_acc: 0.1536\n",
            "Epoch 45/100\n",
            "86/86 - 2s - loss: 1.5622 - acc: 0.1257 - val_loss: 1.5697 - val_acc: 0.0915\n",
            "Epoch 46/100\n",
            "86/86 - 2s - loss: 1.5616 - acc: 0.1119 - val_loss: 1.5652 - val_acc: 0.1536\n",
            "Epoch 47/100\n",
            "86/86 - 2s - loss: 1.5614 - acc: 0.1174 - val_loss: 1.5685 - val_acc: 0.0882\n",
            "Epoch 48/100\n",
            "86/86 - 2s - loss: 1.5603 - acc: 0.1246 - val_loss: 1.5756 - val_acc: 0.0915\n",
            "Epoch 49/100\n",
            "86/86 - 2s - loss: 1.5613 - acc: 0.1221 - val_loss: 1.5674 - val_acc: 0.0915\n",
            "Epoch 50/100\n",
            "86/86 - 2s - loss: 1.5616 - acc: 0.1199 - val_loss: 1.5696 - val_acc: 0.0915\n",
            "Epoch 51/100\n",
            "86/86 - 2s - loss: 1.5618 - acc: 0.1166 - val_loss: 1.5685 - val_acc: 0.0915\n",
            "Epoch 52/100\n",
            "86/86 - 2s - loss: 1.5611 - acc: 0.1217 - val_loss: 1.5660 - val_acc: 0.0882\n",
            "Epoch 53/100\n",
            "86/86 - 2s - loss: 1.5611 - acc: 0.1228 - val_loss: 1.5678 - val_acc: 0.1536\n",
            "Epoch 54/100\n",
            "86/86 - 2s - loss: 1.5624 - acc: 0.1134 - val_loss: 1.5706 - val_acc: 0.1176\n",
            "Epoch 55/100\n",
            "86/86 - 2s - loss: 1.5618 - acc: 0.1221 - val_loss: 1.5674 - val_acc: 0.0915\n",
            "Epoch 56/100\n",
            "86/86 - 2s - loss: 1.5626 - acc: 0.1203 - val_loss: 1.5667 - val_acc: 0.0915\n",
            "Epoch 57/100\n",
            "86/86 - 2s - loss: 1.5622 - acc: 0.1214 - val_loss: 1.5702 - val_acc: 0.0915\n",
            "Epoch 58/100\n",
            "86/86 - 2s - loss: 1.5611 - acc: 0.1221 - val_loss: 1.5761 - val_acc: 0.1536\n",
            "Epoch 59/100\n",
            "86/86 - 2s - loss: 1.5621 - acc: 0.1206 - val_loss: 1.5697 - val_acc: 0.1176\n",
            "Epoch 60/100\n",
            "86/86 - 2s - loss: 1.5604 - acc: 0.1214 - val_loss: 1.5670 - val_acc: 0.1536\n",
            "Epoch 61/100\n",
            "86/86 - 2s - loss: 1.5620 - acc: 0.1170 - val_loss: 1.5667 - val_acc: 0.0915\n",
            "Epoch 62/100\n",
            "86/86 - 2s - loss: 1.5619 - acc: 0.1188 - val_loss: 1.5654 - val_acc: 0.0915\n",
            "Epoch 63/100\n",
            "86/86 - 2s - loss: 1.5612 - acc: 0.1243 - val_loss: 1.5682 - val_acc: 0.0915\n",
            "Epoch 64/100\n",
            "86/86 - 2s - loss: 1.5616 - acc: 0.1199 - val_loss: 1.5920 - val_acc: 0.0915\n",
            "Epoch 65/100\n",
            "86/86 - 2s - loss: 1.5625 - acc: 0.1185 - val_loss: 1.5687 - val_acc: 0.1176\n",
            "Epoch 66/100\n",
            "86/86 - 2s - loss: 1.5620 - acc: 0.1177 - val_loss: 1.5679 - val_acc: 0.1536\n",
            "Epoch 67/100\n",
            "86/86 - 2s - loss: 1.5616 - acc: 0.1239 - val_loss: 1.5671 - val_acc: 0.0915\n",
            "Epoch 68/100\n",
            "86/86 - 2s - loss: 1.5615 - acc: 0.1195 - val_loss: 1.5660 - val_acc: 0.0915\n",
            "Epoch 69/100\n",
            "86/86 - 2s - loss: 1.5617 - acc: 0.1195 - val_loss: 1.5674 - val_acc: 0.0915\n",
            "Epoch 70/100\n",
            "86/86 - 2s - loss: 1.5612 - acc: 0.1181 - val_loss: 1.5684 - val_acc: 0.0915\n",
            "Epoch 71/100\n",
            "86/86 - 2s - loss: 1.5615 - acc: 0.1188 - val_loss: 1.5768 - val_acc: 0.0915\n",
            "Epoch 72/100\n",
            "86/86 - 2s - loss: 1.5623 - acc: 0.1214 - val_loss: 1.5757 - val_acc: 0.0915\n",
            "Epoch 73/100\n",
            "86/86 - 2s - loss: 1.5619 - acc: 0.1195 - val_loss: 1.5722 - val_acc: 0.0915\n",
            "Epoch 74/100\n",
            "86/86 - 2s - loss: 1.5620 - acc: 0.1210 - val_loss: 1.5725 - val_acc: 0.0915\n",
            "Epoch 75/100\n",
            "86/86 - 2s - loss: 1.5614 - acc: 0.1246 - val_loss: 1.5703 - val_acc: 0.0915\n",
            "Epoch 76/100\n",
            "86/86 - 2s - loss: 1.5614 - acc: 0.1181 - val_loss: 1.5682 - val_acc: 0.0915\n",
            "Epoch 77/100\n",
            "86/86 - 2s - loss: 1.5620 - acc: 0.1214 - val_loss: 1.5660 - val_acc: 0.0882\n",
            "Epoch 78/100\n",
            "86/86 - 2s - loss: 1.5622 - acc: 0.1188 - val_loss: 1.5715 - val_acc: 0.0915\n",
            "Epoch 79/100\n",
            "86/86 - 2s - loss: 1.5620 - acc: 0.1214 - val_loss: 1.5749 - val_acc: 0.0915\n",
            "Epoch 80/100\n",
            "86/86 - 2s - loss: 1.5615 - acc: 0.1203 - val_loss: 1.5706 - val_acc: 0.0915\n",
            "Epoch 81/100\n",
            "86/86 - 2s - loss: 1.5611 - acc: 0.1210 - val_loss: 1.5742 - val_acc: 0.0915\n",
            "Epoch 82/100\n",
            "86/86 - 2s - loss: 1.5622 - acc: 0.1206 - val_loss: 1.5772 - val_acc: 0.0915\n",
            "Epoch 83/100\n",
            "86/86 - 2s - loss: 1.5613 - acc: 0.1243 - val_loss: 1.5712 - val_acc: 0.0915\n",
            "Epoch 84/100\n",
            "86/86 - 2s - loss: 1.5608 - acc: 0.1257 - val_loss: 1.5745 - val_acc: 0.1176\n",
            "Epoch 85/100\n",
            "86/86 - 2s - loss: 1.5616 - acc: 0.1210 - val_loss: 1.5813 - val_acc: 0.0882\n",
            "Epoch 86/100\n",
            "86/86 - 2s - loss: 1.5608 - acc: 0.1199 - val_loss: 1.5796 - val_acc: 0.0915\n",
            "Epoch 87/100\n",
            "86/86 - 2s - loss: 1.5615 - acc: 0.1217 - val_loss: 1.5666 - val_acc: 0.0915\n",
            "Epoch 88/100\n",
            "86/86 - 2s - loss: 1.5614 - acc: 0.1210 - val_loss: 1.5646 - val_acc: 0.0915\n",
            "Epoch 89/100\n",
            "86/86 - 2s - loss: 1.5611 - acc: 0.1214 - val_loss: 1.5668 - val_acc: 0.0915\n",
            "Epoch 90/100\n",
            "86/86 - 2s - loss: 1.5610 - acc: 0.1228 - val_loss: 1.5689 - val_acc: 0.0915\n",
            "Epoch 91/100\n",
            "86/86 - 2s - loss: 1.5614 - acc: 0.1192 - val_loss: 1.5717 - val_acc: 0.0915\n",
            "Epoch 92/100\n",
            "86/86 - 2s - loss: 1.5618 - acc: 0.1203 - val_loss: 1.5709 - val_acc: 0.1176\n",
            "Epoch 93/100\n",
            "86/86 - 2s - loss: 1.5611 - acc: 0.1225 - val_loss: 1.5703 - val_acc: 0.0915\n",
            "Epoch 94/100\n",
            "86/86 - 2s - loss: 1.5610 - acc: 0.1239 - val_loss: 1.5723 - val_acc: 0.0915\n",
            "Epoch 95/100\n",
            "86/86 - 2s - loss: 1.5612 - acc: 0.1228 - val_loss: 1.5733 - val_acc: 0.0915\n",
            "Epoch 96/100\n",
            "86/86 - 2s - loss: 1.5622 - acc: 0.1214 - val_loss: 1.5672 - val_acc: 0.0915\n",
            "Epoch 97/100\n",
            "86/86 - 2s - loss: 1.5615 - acc: 0.1235 - val_loss: 1.5665 - val_acc: 0.0915\n",
            "Epoch 98/100\n",
            "86/86 - 2s - loss: 1.5621 - acc: 0.1210 - val_loss: 1.5682 - val_acc: 0.0915\n",
            "Epoch 99/100\n",
            "86/86 - 2s - loss: 1.5613 - acc: 0.1217 - val_loss: 1.5687 - val_acc: 0.0915\n",
            "Epoch 100/100\n",
            "86/86 - 2s - loss: 1.5619 - acc: 0.1174 - val_loss: 1.5652 - val_acc: 0.0915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe0a145cc10>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-w8_FdPhfHb"
      },
      "source": [
        "## Using pretrained embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb2nmRvYo4Uo"
      },
      "source": [
        "As is clear from above, weighing the losses did not help much either. Another possible approach can be to use pretrained embeddings, instead of learning emebeddings as in the approach above so far. The motivation for using these embeddings is as they have been trained on much larger word corpus, they contain much better relational information between different words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HctVl19ijYnZ"
      },
      "source": [
        "There are a few different embedding vector sizes, including 50, 100, 200 and 300 dimensions. Here, we will work with 200 dimensional vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5l134RunyGJ"
      },
      "source": [
        "embedding_dimension = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxUOCgVh_1h"
      },
      "source": [
        "# Download GloVe pretrained embeddings\n",
        "import os\n",
        "if not os.path.isfile(f'glove.6B.{embedding_dimension}d.txt'):\n",
        "  ! wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "  ! unzip glove.6B.zip\n",
        "  ! rm glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWyZI0oRjHc3",
        "outputId": "f30c2c9a-d23d-4626-fc2d-31de7655395c"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open(f'glove.6B.{embedding_dimension}d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD5EoUwmkJeC"
      },
      "source": [
        "# prepare tokenizer\n",
        "embed_tokenize = Tokenizer()\n",
        "embed_tokenize.fit_on_texts(df[\"speech_cleaned\"])\n",
        "vocab_size = len(embed_tokenize.word_index) + 1\n",
        "\n",
        "# create a weight matrix for words in dataset\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dimension))\n",
        "for word, i in embed_tokenize.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mSI1Ih1ldVe"
      },
      "source": [
        "df[\"speech_cleaned\"] = df[\"speech\"].apply(lambda x: \" \".join([re.sub(r'[^\\w\\d]','', item.lower()) for item in x.split() if re.sub(r'[^\\w\\d]','', item.lower()) not in stop_words]))\n",
        "train=df.sample(frac=0.8,random_state=101) #random state is a seed value\n",
        "test=df.drop(train.index).sample(frac=1.0,random_state=45)\n",
        "\n",
        "# set max sequence length\n",
        "max_len = 150\n",
        "# integer encode the speech\n",
        "X_train = pad_sequences(embed_tokenize.texts_to_sequences(train[\"speech_cleaned\"]), maxlen=max_len, padding=\"post\")\n",
        "Y_train = train[speakers]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYzNskCHmdhR",
        "outputId": "d81c673f-fa8a-4745-d4f4-d8bd52edad90"
      },
      "source": [
        "# build a model with GloVe embeddings\n",
        "glove_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dimension, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "    Bidirectional(LSTM(32, return_sequences=True)),\n",
        "    Bidirectional(LSTM(16)),\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(16),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(len(speakers\n",
        "    ), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "glove_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"], loss_weights=loss_weights)\n",
        "glove_model.fit(X_train, Y_train, validation_split=.1, epochs=100, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "86/86 - 7s - loss: 1.7418 - acc: 0.0629 - val_loss: 1.6144 - val_acc: 0.1046\n",
            "Epoch 2/100\n",
            "86/86 - 2s - loss: 1.5694 - acc: 0.1388 - val_loss: 1.5581 - val_acc: 0.1242\n",
            "Epoch 3/100\n",
            "86/86 - 2s - loss: 1.4853 - acc: 0.1868 - val_loss: 1.5011 - val_acc: 0.1438\n",
            "Epoch 4/100\n",
            "86/86 - 2s - loss: 1.4127 - acc: 0.2053 - val_loss: 1.4569 - val_acc: 0.1601\n",
            "Epoch 5/100\n",
            "86/86 - 2s - loss: 1.3778 - acc: 0.2242 - val_loss: 1.3941 - val_acc: 0.2092\n",
            "Epoch 6/100\n",
            "86/86 - 2s - loss: 1.3254 - acc: 0.2504 - val_loss: 1.4003 - val_acc: 0.1895\n",
            "Epoch 7/100\n",
            "86/86 - 2s - loss: 1.2926 - acc: 0.2714 - val_loss: 1.3594 - val_acc: 0.1895\n",
            "Epoch 8/100\n",
            "86/86 - 2s - loss: 1.2341 - acc: 0.3070 - val_loss: 1.3193 - val_acc: 0.2516\n",
            "Epoch 9/100\n",
            "86/86 - 2s - loss: 1.2006 - acc: 0.3161 - val_loss: 1.2954 - val_acc: 0.2712\n",
            "Epoch 10/100\n",
            "86/86 - 2s - loss: 1.1620 - acc: 0.3423 - val_loss: 1.2770 - val_acc: 0.2451\n",
            "Epoch 11/100\n",
            "86/86 - 2s - loss: 1.1195 - acc: 0.3757 - val_loss: 1.2964 - val_acc: 0.2418\n",
            "Epoch 12/100\n",
            "86/86 - 2s - loss: 1.0807 - acc: 0.3899 - val_loss: 1.2999 - val_acc: 0.2582\n",
            "Epoch 13/100\n",
            "86/86 - 2s - loss: 1.0540 - acc: 0.4023 - val_loss: 1.3038 - val_acc: 0.2516\n",
            "Epoch 14/100\n",
            "86/86 - 2s - loss: 1.0033 - acc: 0.4302 - val_loss: 1.3161 - val_acc: 0.2614\n",
            "Epoch 15/100\n",
            "86/86 - 2s - loss: 0.9844 - acc: 0.4542 - val_loss: 1.4108 - val_acc: 0.2549\n",
            "Epoch 16/100\n",
            "86/86 - 2s - loss: 0.9531 - acc: 0.4618 - val_loss: 1.4139 - val_acc: 0.2712\n",
            "Epoch 17/100\n",
            "86/86 - 2s - loss: 0.9072 - acc: 0.4833 - val_loss: 1.4054 - val_acc: 0.2320\n",
            "Epoch 18/100\n",
            "86/86 - 2s - loss: 0.8673 - acc: 0.5131 - val_loss: 1.3461 - val_acc: 0.2810\n",
            "Epoch 19/100\n",
            "86/86 - 2s - loss: 0.8391 - acc: 0.5280 - val_loss: 1.3497 - val_acc: 0.2614\n",
            "Epoch 20/100\n",
            "86/86 - 2s - loss: 0.8123 - acc: 0.5432 - val_loss: 1.4442 - val_acc: 0.2680\n",
            "Epoch 21/100\n",
            "86/86 - 2s - loss: 0.7732 - acc: 0.5560 - val_loss: 1.4450 - val_acc: 0.2614\n",
            "Epoch 22/100\n",
            "86/86 - 2s - loss: 0.7412 - acc: 0.5861 - val_loss: 1.4497 - val_acc: 0.2680\n",
            "Epoch 23/100\n",
            "86/86 - 2s - loss: 0.7193 - acc: 0.5858 - val_loss: 1.4357 - val_acc: 0.2745\n",
            "Epoch 24/100\n",
            "86/86 - 2s - loss: 0.6865 - acc: 0.5952 - val_loss: 1.5010 - val_acc: 0.2451\n",
            "Epoch 25/100\n",
            "86/86 - 2s - loss: 0.6852 - acc: 0.5977 - val_loss: 1.5183 - val_acc: 0.2680\n",
            "Epoch 26/100\n",
            "86/86 - 2s - loss: 0.6646 - acc: 0.6225 - val_loss: 1.5183 - val_acc: 0.2680\n",
            "Epoch 27/100\n",
            "86/86 - 2s - loss: 0.6502 - acc: 0.6272 - val_loss: 1.5409 - val_acc: 0.2843\n",
            "Epoch 28/100\n",
            "86/86 - 2s - loss: 0.6248 - acc: 0.6410 - val_loss: 1.5051 - val_acc: 0.2778\n",
            "Epoch 29/100\n",
            "86/86 - 2s - loss: 0.6195 - acc: 0.6457 - val_loss: 1.5450 - val_acc: 0.2680\n",
            "Epoch 30/100\n",
            "86/86 - 2s - loss: 0.5930 - acc: 0.6621 - val_loss: 1.5625 - val_acc: 0.2778\n",
            "Epoch 31/100\n",
            "86/86 - 2s - loss: 0.5756 - acc: 0.6799 - val_loss: 1.6234 - val_acc: 0.2680\n",
            "Epoch 32/100\n",
            "86/86 - 2s - loss: 0.5499 - acc: 0.6879 - val_loss: 1.7052 - val_acc: 0.2680\n",
            "Epoch 33/100\n",
            "86/86 - 2s - loss: 0.5560 - acc: 0.6824 - val_loss: 1.5905 - val_acc: 0.2876\n",
            "Epoch 34/100\n",
            "86/86 - 2s - loss: 0.5452 - acc: 0.6820 - val_loss: 1.5728 - val_acc: 0.2974\n",
            "Epoch 35/100\n",
            "86/86 - 2s - loss: 0.5037 - acc: 0.7162 - val_loss: 1.7257 - val_acc: 0.3072\n",
            "Epoch 36/100\n",
            "86/86 - 2s - loss: 0.5209 - acc: 0.6948 - val_loss: 1.7601 - val_acc: 0.2810\n",
            "Epoch 37/100\n",
            "86/86 - 2s - loss: 0.5116 - acc: 0.7075 - val_loss: 1.7957 - val_acc: 0.2451\n",
            "Epoch 38/100\n",
            "86/86 - 2s - loss: 0.4899 - acc: 0.7231 - val_loss: 1.9620 - val_acc: 0.3039\n",
            "Epoch 39/100\n",
            "86/86 - 2s - loss: 0.4719 - acc: 0.7257 - val_loss: 1.7946 - val_acc: 0.2745\n",
            "Epoch 40/100\n",
            "86/86 - 2s - loss: 0.4589 - acc: 0.7420 - val_loss: 1.7175 - val_acc: 0.2843\n",
            "Epoch 41/100\n",
            "86/86 - 2s - loss: 0.4699 - acc: 0.7275 - val_loss: 2.0386 - val_acc: 0.2908\n",
            "Epoch 42/100\n",
            "86/86 - 2s - loss: 0.4415 - acc: 0.7547 - val_loss: 1.7797 - val_acc: 0.3039\n",
            "Epoch 43/100\n",
            "86/86 - 2s - loss: 0.4330 - acc: 0.7504 - val_loss: 1.7978 - val_acc: 0.3007\n",
            "Epoch 44/100\n",
            "86/86 - 2s - loss: 0.4331 - acc: 0.7504 - val_loss: 1.9096 - val_acc: 0.2778\n",
            "Epoch 45/100\n",
            "86/86 - 2s - loss: 0.4174 - acc: 0.7674 - val_loss: 1.8674 - val_acc: 0.2941\n",
            "Epoch 46/100\n",
            "86/86 - 2s - loss: 0.4291 - acc: 0.7533 - val_loss: 2.1111 - val_acc: 0.2843\n",
            "Epoch 47/100\n",
            "86/86 - 2s - loss: 0.4242 - acc: 0.7703 - val_loss: 1.9089 - val_acc: 0.2843\n",
            "Epoch 48/100\n",
            "86/86 - 2s - loss: 0.4055 - acc: 0.7631 - val_loss: 1.9858 - val_acc: 0.2745\n",
            "Epoch 49/100\n",
            "86/86 - 2s - loss: 0.3913 - acc: 0.7849 - val_loss: 1.9695 - val_acc: 0.3007\n",
            "Epoch 50/100\n",
            "86/86 - 2s - loss: 0.3909 - acc: 0.7765 - val_loss: 2.1071 - val_acc: 0.2647\n",
            "Epoch 51/100\n",
            "86/86 - 2s - loss: 0.3901 - acc: 0.7783 - val_loss: 2.0593 - val_acc: 0.2941\n",
            "Epoch 52/100\n",
            "86/86 - 2s - loss: 0.4088 - acc: 0.7685 - val_loss: 2.1599 - val_acc: 0.2810\n",
            "Epoch 53/100\n",
            "86/86 - 2s - loss: 0.3874 - acc: 0.7823 - val_loss: 1.9933 - val_acc: 0.2876\n",
            "Epoch 54/100\n",
            "86/86 - 2s - loss: 0.3744 - acc: 0.7900 - val_loss: 1.9336 - val_acc: 0.2941\n",
            "Epoch 55/100\n",
            "86/86 - 2s - loss: 0.3597 - acc: 0.7874 - val_loss: 2.0777 - val_acc: 0.2843\n",
            "Epoch 56/100\n",
            "86/86 - 2s - loss: 0.3454 - acc: 0.8056 - val_loss: 2.1128 - val_acc: 0.2516\n",
            "Epoch 57/100\n",
            "86/86 - 2s - loss: 0.3454 - acc: 0.8045 - val_loss: 2.0363 - val_acc: 0.3039\n",
            "Epoch 58/100\n",
            "86/86 - 2s - loss: 0.3372 - acc: 0.8150 - val_loss: 2.1957 - val_acc: 0.2549\n",
            "Epoch 59/100\n",
            "86/86 - 2s - loss: 0.3527 - acc: 0.8038 - val_loss: 2.2269 - val_acc: 0.2745\n",
            "Epoch 60/100\n",
            "86/86 - 2s - loss: 0.3375 - acc: 0.8085 - val_loss: 2.1742 - val_acc: 0.2484\n",
            "Epoch 61/100\n",
            "86/86 - 2s - loss: 0.3203 - acc: 0.8140 - val_loss: 2.2297 - val_acc: 0.2712\n",
            "Epoch 62/100\n",
            "86/86 - 2s - loss: 0.3277 - acc: 0.8100 - val_loss: 2.1548 - val_acc: 0.3007\n",
            "Epoch 63/100\n",
            "86/86 - 2s - loss: 0.3276 - acc: 0.8125 - val_loss: 2.3182 - val_acc: 0.2941\n",
            "Epoch 64/100\n",
            "86/86 - 2s - loss: 0.3435 - acc: 0.8089 - val_loss: 2.2527 - val_acc: 0.2843\n",
            "Epoch 65/100\n",
            "86/86 - 2s - loss: 0.3288 - acc: 0.8129 - val_loss: 2.1925 - val_acc: 0.2908\n",
            "Epoch 66/100\n",
            "86/86 - 2s - loss: 0.3278 - acc: 0.8245 - val_loss: 2.3767 - val_acc: 0.2843\n",
            "Epoch 67/100\n",
            "86/86 - 2s - loss: 0.3119 - acc: 0.8285 - val_loss: 2.2346 - val_acc: 0.2876\n",
            "Epoch 68/100\n",
            "86/86 - 2s - loss: 0.3073 - acc: 0.8270 - val_loss: 2.2986 - val_acc: 0.2680\n",
            "Epoch 69/100\n",
            "86/86 - 2s - loss: 0.3089 - acc: 0.8227 - val_loss: 2.1979 - val_acc: 0.2974\n",
            "Epoch 70/100\n",
            "86/86 - 2s - loss: 0.3013 - acc: 0.8347 - val_loss: 2.2488 - val_acc: 0.3072\n",
            "Epoch 71/100\n",
            "86/86 - 2s - loss: 0.2958 - acc: 0.8368 - val_loss: 2.2660 - val_acc: 0.2745\n",
            "Epoch 72/100\n",
            "86/86 - 2s - loss: 0.2987 - acc: 0.8263 - val_loss: 2.3743 - val_acc: 0.3039\n",
            "Epoch 73/100\n",
            "86/86 - 2s - loss: 0.2890 - acc: 0.8347 - val_loss: 2.3031 - val_acc: 0.3039\n",
            "Epoch 74/100\n",
            "86/86 - 2s - loss: 0.3030 - acc: 0.8354 - val_loss: 2.5386 - val_acc: 0.3007\n",
            "Epoch 75/100\n",
            "86/86 - 2s - loss: 0.2753 - acc: 0.8510 - val_loss: 2.3629 - val_acc: 0.2712\n",
            "Epoch 76/100\n",
            "86/86 - 2s - loss: 0.2827 - acc: 0.8434 - val_loss: 2.3370 - val_acc: 0.3203\n",
            "Epoch 77/100\n",
            "86/86 - 2s - loss: 0.3021 - acc: 0.8296 - val_loss: 2.2879 - val_acc: 0.2941\n",
            "Epoch 78/100\n",
            "86/86 - 2s - loss: 0.2828 - acc: 0.8387 - val_loss: 2.2852 - val_acc: 0.3105\n",
            "Epoch 79/100\n",
            "86/86 - 2s - loss: 0.2746 - acc: 0.8394 - val_loss: 2.4731 - val_acc: 0.2778\n",
            "Epoch 80/100\n",
            "86/86 - 2s - loss: 0.2739 - acc: 0.8448 - val_loss: 2.4429 - val_acc: 0.3039\n",
            "Epoch 81/100\n",
            "86/86 - 2s - loss: 0.2686 - acc: 0.8499 - val_loss: 2.4912 - val_acc: 0.3170\n",
            "Epoch 82/100\n",
            "86/86 - 2s - loss: 0.2804 - acc: 0.8427 - val_loss: 2.5579 - val_acc: 0.2843\n",
            "Epoch 83/100\n",
            "86/86 - 2s - loss: 0.2771 - acc: 0.8452 - val_loss: 2.5749 - val_acc: 0.2614\n",
            "Epoch 84/100\n",
            "86/86 - 2s - loss: 0.2691 - acc: 0.8445 - val_loss: 2.4674 - val_acc: 0.2712\n",
            "Epoch 85/100\n",
            "86/86 - 2s - loss: 0.2680 - acc: 0.8477 - val_loss: 2.5559 - val_acc: 0.2778\n",
            "Epoch 86/100\n",
            "86/86 - 2s - loss: 0.2665 - acc: 0.8419 - val_loss: 2.5523 - val_acc: 0.2908\n",
            "Epoch 87/100\n",
            "86/86 - 2s - loss: 0.2527 - acc: 0.8579 - val_loss: 2.6040 - val_acc: 0.2974\n",
            "Epoch 88/100\n",
            "86/86 - 2s - loss: 0.2568 - acc: 0.8550 - val_loss: 2.5762 - val_acc: 0.2876\n",
            "Epoch 89/100\n",
            "86/86 - 2s - loss: 0.2595 - acc: 0.8528 - val_loss: 2.6258 - val_acc: 0.2647\n",
            "Epoch 90/100\n",
            "86/86 - 2s - loss: 0.2450 - acc: 0.8634 - val_loss: 2.6267 - val_acc: 0.2810\n",
            "Epoch 91/100\n",
            "86/86 - 2s - loss: 0.2604 - acc: 0.8532 - val_loss: 2.6965 - val_acc: 0.2778\n",
            "Epoch 92/100\n",
            "86/86 - 2s - loss: 0.2577 - acc: 0.8532 - val_loss: 2.6531 - val_acc: 0.3105\n",
            "Epoch 93/100\n",
            "86/86 - 2s - loss: 0.2446 - acc: 0.8597 - val_loss: 2.6203 - val_acc: 0.2810\n",
            "Epoch 94/100\n",
            "86/86 - 2s - loss: 0.2360 - acc: 0.8670 - val_loss: 2.5928 - val_acc: 0.2745\n",
            "Epoch 95/100\n",
            "86/86 - 2s - loss: 0.2434 - acc: 0.8645 - val_loss: 2.6414 - val_acc: 0.2974\n",
            "Epoch 96/100\n",
            "86/86 - 2s - loss: 0.2384 - acc: 0.8652 - val_loss: 2.7429 - val_acc: 0.2451\n",
            "Epoch 97/100\n",
            "86/86 - 2s - loss: 0.2485 - acc: 0.8645 - val_loss: 2.6616 - val_acc: 0.2810\n",
            "Epoch 98/100\n",
            "86/86 - 2s - loss: 0.2295 - acc: 0.8728 - val_loss: 2.8754 - val_acc: 0.2843\n",
            "Epoch 99/100\n",
            "86/86 - 2s - loss: 0.2342 - acc: 0.8579 - val_loss: 2.5934 - val_acc: 0.3105\n",
            "Epoch 100/100\n",
            "86/86 - 2s - loss: 0.2262 - acc: 0.8641 - val_loss: 2.7309 - val_acc: 0.2876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe08d3b5d50>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEIM0sfToSdY"
      },
      "source": [
        "* It is immidiately clear that using the pretrained embeddings help tremendously.\n",
        "* In cases, where the model is supposed to be for a very specific task, we can also retrain these embeddings for better adaptability to a given task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nwi7-mgonNc",
        "outputId": "da8a4782-b3b8-422e-9b89-9de0c7de482f"
      },
      "source": [
        "# build a model with GloVe embeddings (trainable = True)\n",
        "glove_model_2 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dimension, weights=[embedding_matrix], input_length=max_len, trainable=True),\n",
        "    Bidirectional(LSTM(32, return_sequences=True)),\n",
        "    Bidirectional(LSTM(16)),\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(16),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(len(speakers\n",
        "    ), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "glove_model_2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"], loss_weights=loss_weights)\n",
        "glove_model_2.fit(X_train, Y_train, validation_split=.1, epochs=100, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "86/86 - 8s - loss: 1.7149 - acc: 0.0974 - val_loss: 1.6177 - val_acc: 0.1536\n",
            "Epoch 2/100\n",
            "86/86 - 2s - loss: 1.5209 - acc: 0.1657 - val_loss: 1.5746 - val_acc: 0.1503\n",
            "Epoch 3/100\n",
            "86/86 - 2s - loss: 1.4038 - acc: 0.2340 - val_loss: 1.5089 - val_acc: 0.1863\n",
            "Epoch 4/100\n",
            "86/86 - 2s - loss: 1.3145 - acc: 0.2856 - val_loss: 1.4385 - val_acc: 0.2582\n",
            "Epoch 5/100\n",
            "86/86 - 2s - loss: 1.2162 - acc: 0.3528 - val_loss: 1.3542 - val_acc: 0.2451\n",
            "Epoch 6/100\n",
            "86/86 - 2s - loss: 1.1161 - acc: 0.4095 - val_loss: 1.3327 - val_acc: 0.2549\n",
            "Epoch 7/100\n",
            "86/86 - 2s - loss: 1.0516 - acc: 0.4379 - val_loss: 1.2890 - val_acc: 0.3301\n",
            "Epoch 8/100\n",
            "86/86 - 2s - loss: 0.9779 - acc: 0.4713 - val_loss: 1.2138 - val_acc: 0.3562\n",
            "Epoch 9/100\n",
            "86/86 - 2s - loss: 0.8838 - acc: 0.5265 - val_loss: 1.2312 - val_acc: 0.3170\n",
            "Epoch 10/100\n",
            "86/86 - 2s - loss: 0.8241 - acc: 0.5552 - val_loss: 1.1923 - val_acc: 0.3399\n",
            "Epoch 11/100\n",
            "86/86 - 2s - loss: 0.7581 - acc: 0.6032 - val_loss: 1.2557 - val_acc: 0.3791\n",
            "Epoch 12/100\n",
            "86/86 - 2s - loss: 0.6966 - acc: 0.6268 - val_loss: 1.2767 - val_acc: 0.3431\n",
            "Epoch 13/100\n",
            "86/86 - 2s - loss: 0.6439 - acc: 0.6606 - val_loss: 1.2878 - val_acc: 0.3333\n",
            "Epoch 14/100\n",
            "86/86 - 2s - loss: 0.6008 - acc: 0.6864 - val_loss: 1.3373 - val_acc: 0.3235\n",
            "Epoch 15/100\n",
            "86/86 - 2s - loss: 0.5561 - acc: 0.7024 - val_loss: 1.2956 - val_acc: 0.3399\n",
            "Epoch 16/100\n",
            "86/86 - 2s - loss: 0.5355 - acc: 0.7249 - val_loss: 1.3039 - val_acc: 0.3497\n",
            "Epoch 17/100\n",
            "86/86 - 2s - loss: 0.5129 - acc: 0.7278 - val_loss: 1.3936 - val_acc: 0.3203\n",
            "Epoch 18/100\n",
            "86/86 - 2s - loss: 0.4652 - acc: 0.7627 - val_loss: 1.3840 - val_acc: 0.3301\n",
            "Epoch 19/100\n",
            "86/86 - 2s - loss: 0.4480 - acc: 0.7609 - val_loss: 1.5496 - val_acc: 0.3333\n",
            "Epoch 20/100\n",
            "86/86 - 2s - loss: 0.4348 - acc: 0.7733 - val_loss: 1.4897 - val_acc: 0.3431\n",
            "Epoch 21/100\n",
            "86/86 - 2s - loss: 0.3990 - acc: 0.7907 - val_loss: 1.6169 - val_acc: 0.3039\n",
            "Epoch 22/100\n",
            "86/86 - 2s - loss: 0.3935 - acc: 0.7998 - val_loss: 1.5571 - val_acc: 0.3235\n",
            "Epoch 23/100\n",
            "86/86 - 2s - loss: 0.3758 - acc: 0.8092 - val_loss: 1.5770 - val_acc: 0.3072\n",
            "Epoch 24/100\n",
            "86/86 - 2s - loss: 0.3744 - acc: 0.8074 - val_loss: 1.5479 - val_acc: 0.3268\n",
            "Epoch 25/100\n",
            "86/86 - 2s - loss: 0.3666 - acc: 0.8107 - val_loss: 1.6296 - val_acc: 0.3301\n",
            "Epoch 26/100\n",
            "86/86 - 2s - loss: 0.3534 - acc: 0.8110 - val_loss: 1.6945 - val_acc: 0.3137\n",
            "Epoch 27/100\n",
            "86/86 - 2s - loss: 0.3330 - acc: 0.8259 - val_loss: 1.6619 - val_acc: 0.3497\n",
            "Epoch 28/100\n",
            "86/86 - 2s - loss: 0.3239 - acc: 0.8299 - val_loss: 1.6443 - val_acc: 0.3431\n",
            "Epoch 29/100\n",
            "86/86 - 2s - loss: 0.3183 - acc: 0.8361 - val_loss: 1.6452 - val_acc: 0.3562\n",
            "Epoch 30/100\n",
            "86/86 - 2s - loss: 0.3000 - acc: 0.8408 - val_loss: 1.6707 - val_acc: 0.3529\n",
            "Epoch 31/100\n",
            "86/86 - 2s - loss: 0.2972 - acc: 0.8470 - val_loss: 1.7175 - val_acc: 0.3203\n",
            "Epoch 32/100\n",
            "86/86 - 2s - loss: 0.2900 - acc: 0.8496 - val_loss: 1.7096 - val_acc: 0.3529\n",
            "Epoch 33/100\n",
            "86/86 - 2s - loss: 0.2838 - acc: 0.8492 - val_loss: 1.6837 - val_acc: 0.3562\n",
            "Epoch 34/100\n",
            "86/86 - 2s - loss: 0.2842 - acc: 0.8561 - val_loss: 1.9957 - val_acc: 0.3431\n",
            "Epoch 35/100\n",
            "86/86 - 2s - loss: 0.2743 - acc: 0.8568 - val_loss: 1.8651 - val_acc: 0.3072\n",
            "Epoch 36/100\n",
            "86/86 - 2s - loss: 0.2624 - acc: 0.8550 - val_loss: 1.8271 - val_acc: 0.3301\n",
            "Epoch 37/100\n",
            "86/86 - 2s - loss: 0.2623 - acc: 0.8547 - val_loss: 1.9269 - val_acc: 0.3268\n",
            "Epoch 38/100\n",
            "86/86 - 2s - loss: 0.2677 - acc: 0.8572 - val_loss: 1.7984 - val_acc: 0.3562\n",
            "Epoch 39/100\n",
            "86/86 - 2s - loss: 0.2601 - acc: 0.8659 - val_loss: 1.8957 - val_acc: 0.3627\n",
            "Epoch 40/100\n",
            "86/86 - 2s - loss: 0.2522 - acc: 0.8732 - val_loss: 1.8813 - val_acc: 0.3399\n",
            "Epoch 41/100\n",
            "86/86 - 2s - loss: 0.2475 - acc: 0.8663 - val_loss: 1.9551 - val_acc: 0.3268\n",
            "Epoch 42/100\n",
            "86/86 - 2s - loss: 0.2498 - acc: 0.8677 - val_loss: 1.9414 - val_acc: 0.3529\n",
            "Epoch 43/100\n",
            "86/86 - 2s - loss: 0.2593 - acc: 0.8619 - val_loss: 2.0461 - val_acc: 0.3399\n",
            "Epoch 44/100\n",
            "86/86 - 2s - loss: 0.2544 - acc: 0.8572 - val_loss: 1.9878 - val_acc: 0.3562\n",
            "Epoch 45/100\n",
            "86/86 - 2s - loss: 0.2386 - acc: 0.8666 - val_loss: 2.0558 - val_acc: 0.3301\n",
            "Epoch 46/100\n",
            "86/86 - 2s - loss: 0.2194 - acc: 0.8841 - val_loss: 2.0794 - val_acc: 0.3268\n",
            "Epoch 47/100\n",
            "86/86 - 2s - loss: 0.2215 - acc: 0.8826 - val_loss: 2.1110 - val_acc: 0.3301\n",
            "Epoch 48/100\n",
            "86/86 - 2s - loss: 0.2115 - acc: 0.8906 - val_loss: 2.1215 - val_acc: 0.3431\n",
            "Epoch 49/100\n",
            "86/86 - 2s - loss: 0.2153 - acc: 0.8844 - val_loss: 2.1822 - val_acc: 0.3497\n",
            "Epoch 50/100\n",
            "86/86 - 2s - loss: 0.2222 - acc: 0.8834 - val_loss: 2.1516 - val_acc: 0.3497\n",
            "Epoch 51/100\n",
            "86/86 - 2s - loss: 0.2192 - acc: 0.8815 - val_loss: 2.0228 - val_acc: 0.3399\n",
            "Epoch 52/100\n",
            "86/86 - 2s - loss: 0.2122 - acc: 0.8819 - val_loss: 2.0868 - val_acc: 0.3333\n",
            "Epoch 53/100\n",
            "86/86 - 2s - loss: 0.2062 - acc: 0.8859 - val_loss: 2.1235 - val_acc: 0.3333\n",
            "Epoch 54/100\n",
            "86/86 - 2s - loss: 0.2196 - acc: 0.8852 - val_loss: 2.2099 - val_acc: 0.3627\n",
            "Epoch 55/100\n",
            "86/86 - 2s - loss: 0.2243 - acc: 0.8772 - val_loss: 2.1025 - val_acc: 0.3660\n",
            "Epoch 56/100\n",
            "86/86 - 2s - loss: 0.2023 - acc: 0.8852 - val_loss: 2.1350 - val_acc: 0.3497\n",
            "Epoch 57/100\n",
            "86/86 - 2s - loss: 0.2019 - acc: 0.8866 - val_loss: 2.1110 - val_acc: 0.3791\n",
            "Epoch 58/100\n",
            "86/86 - 2s - loss: 0.2041 - acc: 0.8881 - val_loss: 2.2111 - val_acc: 0.3693\n",
            "Epoch 59/100\n",
            "86/86 - 2s - loss: 0.2023 - acc: 0.8855 - val_loss: 2.2857 - val_acc: 0.3725\n",
            "Epoch 60/100\n",
            "86/86 - 2s - loss: 0.1944 - acc: 0.8935 - val_loss: 2.2184 - val_acc: 0.3824\n",
            "Epoch 61/100\n",
            "86/86 - 2s - loss: 0.2004 - acc: 0.8888 - val_loss: 2.3006 - val_acc: 0.3856\n",
            "Epoch 62/100\n",
            "86/86 - 2s - loss: 0.1953 - acc: 0.8906 - val_loss: 2.3589 - val_acc: 0.3497\n",
            "Epoch 63/100\n",
            "86/86 - 2s - loss: 0.1939 - acc: 0.8881 - val_loss: 2.2555 - val_acc: 0.3725\n",
            "Epoch 64/100\n",
            "86/86 - 2s - loss: 0.1955 - acc: 0.8884 - val_loss: 2.3265 - val_acc: 0.3464\n",
            "Epoch 65/100\n",
            "86/86 - 2s - loss: 0.1985 - acc: 0.8921 - val_loss: 2.3583 - val_acc: 0.3595\n",
            "Epoch 66/100\n",
            "86/86 - 2s - loss: 0.1872 - acc: 0.8906 - val_loss: 2.3820 - val_acc: 0.3627\n",
            "Epoch 67/100\n",
            "86/86 - 2s - loss: 0.1826 - acc: 0.8946 - val_loss: 2.4199 - val_acc: 0.3824\n",
            "Epoch 68/100\n",
            "86/86 - 2s - loss: 0.1901 - acc: 0.8914 - val_loss: 2.3958 - val_acc: 0.3431\n",
            "Epoch 69/100\n",
            "86/86 - 2s - loss: 0.2134 - acc: 0.8870 - val_loss: 2.6104 - val_acc: 0.3431\n",
            "Epoch 70/100\n",
            "86/86 - 2s - loss: 0.1993 - acc: 0.8921 - val_loss: 2.5615 - val_acc: 0.3497\n",
            "Epoch 71/100\n",
            "86/86 - 2s - loss: 0.1864 - acc: 0.8953 - val_loss: 2.4955 - val_acc: 0.3497\n",
            "Epoch 72/100\n",
            "86/86 - 2s - loss: 0.1832 - acc: 0.8935 - val_loss: 2.6701 - val_acc: 0.3464\n",
            "Epoch 73/100\n",
            "86/86 - 2s - loss: 0.1751 - acc: 0.8979 - val_loss: 2.5241 - val_acc: 0.3791\n",
            "Epoch 74/100\n",
            "86/86 - 2s - loss: 0.1901 - acc: 0.8899 - val_loss: 2.4814 - val_acc: 0.3562\n",
            "Epoch 75/100\n",
            "86/86 - 2s - loss: 0.1818 - acc: 0.8975 - val_loss: 2.5521 - val_acc: 0.3693\n",
            "Epoch 76/100\n",
            "86/86 - 2s - loss: 0.1715 - acc: 0.9044 - val_loss: 2.6765 - val_acc: 0.3529\n",
            "Epoch 77/100\n",
            "86/86 - 2s - loss: 0.1684 - acc: 0.9077 - val_loss: 2.5247 - val_acc: 0.3758\n",
            "Epoch 78/100\n",
            "86/86 - 2s - loss: 0.1824 - acc: 0.8917 - val_loss: 2.7434 - val_acc: 0.3137\n",
            "Epoch 79/100\n",
            "86/86 - 2s - loss: 0.1749 - acc: 0.8972 - val_loss: 2.7455 - val_acc: 0.3497\n",
            "Epoch 80/100\n",
            "86/86 - 2s - loss: 0.1795 - acc: 0.9023 - val_loss: 2.8835 - val_acc: 0.3301\n",
            "Epoch 81/100\n",
            "86/86 - 2s - loss: 0.1699 - acc: 0.9030 - val_loss: 2.6783 - val_acc: 0.3562\n",
            "Epoch 82/100\n",
            "86/86 - 2s - loss: 0.1842 - acc: 0.8979 - val_loss: 2.8364 - val_acc: 0.3170\n",
            "Epoch 83/100\n",
            "86/86 - 2s - loss: 0.1800 - acc: 0.8990 - val_loss: 2.5288 - val_acc: 0.3562\n",
            "Epoch 84/100\n",
            "86/86 - 2s - loss: 0.1634 - acc: 0.9059 - val_loss: 2.8000 - val_acc: 0.3627\n",
            "Epoch 85/100\n",
            "86/86 - 2s - loss: 0.1603 - acc: 0.9110 - val_loss: 2.7367 - val_acc: 0.3660\n",
            "Epoch 86/100\n",
            "86/86 - 2s - loss: 0.1749 - acc: 0.8997 - val_loss: 2.7362 - val_acc: 0.3562\n",
            "Epoch 87/100\n",
            "86/86 - 2s - loss: 0.1668 - acc: 0.9106 - val_loss: 2.7689 - val_acc: 0.3562\n",
            "Epoch 88/100\n",
            "86/86 - 2s - loss: 0.1738 - acc: 0.8964 - val_loss: 2.6517 - val_acc: 0.3399\n",
            "Epoch 89/100\n",
            "86/86 - 2s - loss: 0.1606 - acc: 0.9102 - val_loss: 2.7320 - val_acc: 0.3562\n",
            "Epoch 90/100\n",
            "86/86 - 2s - loss: 0.1661 - acc: 0.9019 - val_loss: 2.7272 - val_acc: 0.3431\n",
            "Epoch 91/100\n",
            "86/86 - 2s - loss: 0.1732 - acc: 0.8979 - val_loss: 2.8921 - val_acc: 0.3399\n",
            "Epoch 92/100\n",
            "86/86 - 2s - loss: 0.1679 - acc: 0.8997 - val_loss: 2.9020 - val_acc: 0.3268\n",
            "Epoch 93/100\n",
            "86/86 - 2s - loss: 0.1718 - acc: 0.9008 - val_loss: 3.1083 - val_acc: 0.3497\n",
            "Epoch 94/100\n",
            "86/86 - 2s - loss: 0.1756 - acc: 0.8957 - val_loss: 2.5858 - val_acc: 0.3856\n",
            "Epoch 95/100\n",
            "86/86 - 2s - loss: 0.1762 - acc: 0.9008 - val_loss: 2.6056 - val_acc: 0.3529\n",
            "Epoch 96/100\n",
            "86/86 - 2s - loss: 0.1796 - acc: 0.9012 - val_loss: 2.6492 - val_acc: 0.3758\n",
            "Epoch 97/100\n",
            "86/86 - 2s - loss: 0.1632 - acc: 0.9048 - val_loss: 2.6583 - val_acc: 0.3464\n",
            "Epoch 98/100\n",
            "86/86 - 2s - loss: 0.1624 - acc: 0.9077 - val_loss: 2.8160 - val_acc: 0.3758\n",
            "Epoch 99/100\n",
            "86/86 - 2s - loss: 0.1600 - acc: 0.9008 - val_loss: 2.8478 - val_acc: 0.3627\n",
            "Epoch 100/100\n",
            "86/86 - 2s - loss: 0.1641 - acc: 0.9015 - val_loss: 2.7577 - val_acc: 0.3529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe08b828990>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65qHryU6qE3X"
      },
      "source": [
        "## Deal with overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nNryi8Fpfkc"
      },
      "source": [
        "* As expected, retraining/finetuning the embeddings result in further gain performance\n",
        "* Now, we have a clear case of overfitting (train acc. >> val.acc.). Some strategies that can help are\n",
        "  * increasing the model capacity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6CJedpwqiW5"
      },
      "source": [
        "### Increasing the model capacity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIBupTRDqCu0",
        "outputId": "3162bada-349b-42da-9f3d-31d46c3df62d"
      },
      "source": [
        "# build a model with GloVe embeddings (trainable = True)\n",
        "glove_model_3 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dimension, weights=[embedding_matrix], input_length=max_len, trainable=True),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(256),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(len(speakers\n",
        "    ), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "glove_model_3.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"], loss_weights=loss_weights)\n",
        "glove_model_3.fit(X_train, Y_train, validation_split=.1, epochs=100, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "86/86 - 8s - loss: 1.6550 - acc: 0.1243 - val_loss: 1.5818 - val_acc: 0.1601\n",
            "Epoch 2/100\n",
            "86/86 - 2s - loss: 1.4136 - acc: 0.2406 - val_loss: 1.5329 - val_acc: 0.2059\n",
            "Epoch 3/100\n",
            "86/86 - 2s - loss: 1.2767 - acc: 0.3172 - val_loss: 1.4374 - val_acc: 0.2647\n",
            "Epoch 4/100\n",
            "86/86 - 2s - loss: 1.1475 - acc: 0.3819 - val_loss: 1.3503 - val_acc: 0.3039\n",
            "Epoch 5/100\n",
            "86/86 - 2s - loss: 1.0310 - acc: 0.4433 - val_loss: 1.4671 - val_acc: 0.2810\n",
            "Epoch 6/100\n",
            "86/86 - 2s - loss: 0.9271 - acc: 0.4815 - val_loss: 1.4655 - val_acc: 0.2582\n",
            "Epoch 7/100\n",
            "86/86 - 2s - loss: 0.8318 - acc: 0.5461 - val_loss: 1.2361 - val_acc: 0.3039\n",
            "Epoch 8/100\n",
            "86/86 - 2s - loss: 0.7406 - acc: 0.6047 - val_loss: 1.1746 - val_acc: 0.3660\n",
            "Epoch 9/100\n",
            "86/86 - 2s - loss: 0.6672 - acc: 0.6319 - val_loss: 1.2056 - val_acc: 0.3595\n",
            "Epoch 10/100\n",
            "86/86 - 2s - loss: 0.6005 - acc: 0.6860 - val_loss: 1.3624 - val_acc: 0.3464\n",
            "Epoch 11/100\n",
            "86/86 - 2s - loss: 0.5124 - acc: 0.7420 - val_loss: 1.2036 - val_acc: 0.3824\n",
            "Epoch 12/100\n",
            "86/86 - 2s - loss: 0.4586 - acc: 0.7671 - val_loss: 1.3220 - val_acc: 0.3627\n",
            "Epoch 13/100\n",
            "86/86 - 2s - loss: 0.4083 - acc: 0.7972 - val_loss: 1.2803 - val_acc: 0.3889\n",
            "Epoch 14/100\n",
            "86/86 - 2s - loss: 0.3897 - acc: 0.8012 - val_loss: 1.2528 - val_acc: 0.4216\n",
            "Epoch 15/100\n",
            "86/86 - 2s - loss: 0.3417 - acc: 0.8270 - val_loss: 1.2927 - val_acc: 0.4085\n",
            "Epoch 16/100\n",
            "86/86 - 2s - loss: 0.3346 - acc: 0.8303 - val_loss: 1.4508 - val_acc: 0.3497\n",
            "Epoch 17/100\n",
            "86/86 - 2s - loss: 0.3039 - acc: 0.8525 - val_loss: 1.3591 - val_acc: 0.3922\n",
            "Epoch 18/100\n",
            "86/86 - 2s - loss: 0.2939 - acc: 0.8496 - val_loss: 1.5551 - val_acc: 0.3856\n",
            "Epoch 19/100\n",
            "86/86 - 2s - loss: 0.2653 - acc: 0.8699 - val_loss: 1.4836 - val_acc: 0.3758\n",
            "Epoch 20/100\n",
            "86/86 - 2s - loss: 0.2543 - acc: 0.8695 - val_loss: 1.7060 - val_acc: 0.3203\n",
            "Epoch 21/100\n",
            "86/86 - 2s - loss: 0.2417 - acc: 0.8754 - val_loss: 1.5239 - val_acc: 0.3660\n",
            "Epoch 22/100\n",
            "86/86 - 2s - loss: 0.2264 - acc: 0.8848 - val_loss: 1.6504 - val_acc: 0.3660\n",
            "Epoch 23/100\n",
            "86/86 - 2s - loss: 0.2336 - acc: 0.8797 - val_loss: 1.6531 - val_acc: 0.3824\n",
            "Epoch 24/100\n",
            "86/86 - 2s - loss: 0.2147 - acc: 0.8950 - val_loss: 1.6153 - val_acc: 0.3922\n",
            "Epoch 25/100\n",
            "86/86 - 2s - loss: 0.2018 - acc: 0.8950 - val_loss: 1.5723 - val_acc: 0.4248\n",
            "Epoch 26/100\n",
            "86/86 - 2s - loss: 0.1958 - acc: 0.9019 - val_loss: 1.6830 - val_acc: 0.3693\n",
            "Epoch 27/100\n",
            "86/86 - 2s - loss: 0.1934 - acc: 0.8986 - val_loss: 1.5286 - val_acc: 0.3954\n",
            "Epoch 28/100\n",
            "86/86 - 2s - loss: 0.1790 - acc: 0.9113 - val_loss: 1.6338 - val_acc: 0.4020\n",
            "Epoch 29/100\n",
            "86/86 - 2s - loss: 0.1787 - acc: 0.9073 - val_loss: 1.7013 - val_acc: 0.3954\n",
            "Epoch 30/100\n",
            "86/86 - 2s - loss: 0.1725 - acc: 0.9106 - val_loss: 1.6737 - val_acc: 0.4020\n",
            "Epoch 31/100\n",
            "86/86 - 2s - loss: 0.1735 - acc: 0.9153 - val_loss: 1.8340 - val_acc: 0.3954\n",
            "Epoch 32/100\n",
            "86/86 - 2s - loss: 0.1885 - acc: 0.8957 - val_loss: 1.8754 - val_acc: 0.3856\n",
            "Epoch 33/100\n",
            "86/86 - 2s - loss: 0.1863 - acc: 0.9041 - val_loss: 1.7955 - val_acc: 0.4085\n",
            "Epoch 34/100\n",
            "86/86 - 2s - loss: 0.1728 - acc: 0.9106 - val_loss: 1.8515 - val_acc: 0.3987\n",
            "Epoch 35/100\n",
            "86/86 - 2s - loss: 0.1683 - acc: 0.9070 - val_loss: 1.8651 - val_acc: 0.3889\n",
            "Epoch 36/100\n",
            "86/86 - 2s - loss: 0.1560 - acc: 0.9172 - val_loss: 1.8537 - val_acc: 0.4085\n",
            "Epoch 37/100\n",
            "86/86 - 2s - loss: 0.1471 - acc: 0.9215 - val_loss: 1.7851 - val_acc: 0.4281\n",
            "Epoch 38/100\n",
            "86/86 - 2s - loss: 0.1414 - acc: 0.9226 - val_loss: 1.8823 - val_acc: 0.4183\n",
            "Epoch 39/100\n",
            "86/86 - 2s - loss: 0.1535 - acc: 0.9190 - val_loss: 1.7882 - val_acc: 0.3954\n",
            "Epoch 40/100\n",
            "86/86 - 2s - loss: 0.1529 - acc: 0.9142 - val_loss: 1.9161 - val_acc: 0.3889\n",
            "Epoch 41/100\n",
            "86/86 - 2s - loss: 0.1459 - acc: 0.9244 - val_loss: 1.9058 - val_acc: 0.3856\n",
            "Epoch 42/100\n",
            "86/86 - 2s - loss: 0.1425 - acc: 0.9193 - val_loss: 1.9315 - val_acc: 0.3889\n",
            "Epoch 43/100\n",
            "86/86 - 2s - loss: 0.1418 - acc: 0.9262 - val_loss: 1.9330 - val_acc: 0.4118\n",
            "Epoch 44/100\n",
            "86/86 - 2s - loss: 0.1476 - acc: 0.9197 - val_loss: 2.0294 - val_acc: 0.3693\n",
            "Epoch 45/100\n",
            "86/86 - 2s - loss: 0.1606 - acc: 0.9132 - val_loss: 1.9800 - val_acc: 0.4020\n",
            "Epoch 46/100\n",
            "86/86 - 2s - loss: 0.1510 - acc: 0.9172 - val_loss: 1.9649 - val_acc: 0.4183\n",
            "Epoch 47/100\n",
            "86/86 - 2s - loss: 0.1435 - acc: 0.9255 - val_loss: 1.9257 - val_acc: 0.3660\n",
            "Epoch 48/100\n",
            "86/86 - 2s - loss: 0.1427 - acc: 0.9255 - val_loss: 1.8897 - val_acc: 0.4085\n",
            "Epoch 49/100\n",
            "86/86 - 2s - loss: 0.1366 - acc: 0.9237 - val_loss: 1.9648 - val_acc: 0.4020\n",
            "Epoch 50/100\n",
            "86/86 - 2s - loss: 0.1305 - acc: 0.9291 - val_loss: 1.8173 - val_acc: 0.4444\n",
            "Epoch 51/100\n",
            "86/86 - 2s - loss: 0.1352 - acc: 0.9219 - val_loss: 1.8009 - val_acc: 0.4183\n",
            "Epoch 52/100\n",
            "86/86 - 2s - loss: 0.1270 - acc: 0.9310 - val_loss: 1.8016 - val_acc: 0.4314\n",
            "Epoch 53/100\n",
            "86/86 - 2s - loss: 0.1401 - acc: 0.9208 - val_loss: 1.7940 - val_acc: 0.4150\n",
            "Epoch 54/100\n",
            "86/86 - 2s - loss: 0.1386 - acc: 0.9237 - val_loss: 2.0102 - val_acc: 0.3824\n",
            "Epoch 55/100\n",
            "86/86 - 2s - loss: 0.1505 - acc: 0.9164 - val_loss: 2.0355 - val_acc: 0.3856\n",
            "Epoch 56/100\n",
            "86/86 - 2s - loss: 0.1285 - acc: 0.9255 - val_loss: 2.0705 - val_acc: 0.3627\n",
            "Epoch 57/100\n",
            "86/86 - 2s - loss: 0.1294 - acc: 0.9262 - val_loss: 2.0379 - val_acc: 0.3987\n",
            "Epoch 58/100\n",
            "86/86 - 2s - loss: 0.1220 - acc: 0.9288 - val_loss: 2.2060 - val_acc: 0.3824\n",
            "Epoch 59/100\n",
            "86/86 - 2s - loss: 0.1259 - acc: 0.9306 - val_loss: 2.1022 - val_acc: 0.4020\n",
            "Epoch 60/100\n",
            "86/86 - 2s - loss: 0.1196 - acc: 0.9277 - val_loss: 2.0472 - val_acc: 0.4020\n",
            "Epoch 61/100\n",
            "86/86 - 2s - loss: 0.1202 - acc: 0.9306 - val_loss: 2.1891 - val_acc: 0.3889\n",
            "Epoch 62/100\n",
            "86/86 - 2s - loss: 0.1201 - acc: 0.9299 - val_loss: 2.1675 - val_acc: 0.3791\n",
            "Epoch 63/100\n",
            "86/86 - 2s - loss: 0.1190 - acc: 0.9244 - val_loss: 2.1675 - val_acc: 0.3889\n",
            "Epoch 64/100\n",
            "86/86 - 2s - loss: 0.1154 - acc: 0.9346 - val_loss: 2.1942 - val_acc: 0.3954\n",
            "Epoch 65/100\n",
            "86/86 - 2s - loss: 0.1144 - acc: 0.9357 - val_loss: 2.1653 - val_acc: 0.3399\n",
            "Epoch 66/100\n",
            "86/86 - 2s - loss: 0.1207 - acc: 0.9306 - val_loss: 2.1769 - val_acc: 0.3824\n",
            "Epoch 67/100\n",
            "86/86 - 2s - loss: 0.1148 - acc: 0.9299 - val_loss: 2.2523 - val_acc: 0.3987\n",
            "Epoch 68/100\n",
            "86/86 - 2s - loss: 0.1154 - acc: 0.9295 - val_loss: 2.3509 - val_acc: 0.3627\n",
            "Epoch 69/100\n",
            "86/86 - 2s - loss: 0.1297 - acc: 0.9277 - val_loss: 2.2713 - val_acc: 0.4020\n",
            "Epoch 70/100\n",
            "86/86 - 2s - loss: 0.1168 - acc: 0.9270 - val_loss: 2.2500 - val_acc: 0.4052\n",
            "Epoch 71/100\n",
            "86/86 - 2s - loss: 0.1212 - acc: 0.9270 - val_loss: 2.2487 - val_acc: 0.4150\n",
            "Epoch 72/100\n",
            "86/86 - 2s - loss: 0.1275 - acc: 0.9237 - val_loss: 2.2908 - val_acc: 0.3791\n",
            "Epoch 73/100\n",
            "86/86 - 2s - loss: 0.1163 - acc: 0.9317 - val_loss: 2.3374 - val_acc: 0.4052\n",
            "Epoch 74/100\n",
            "86/86 - 2s - loss: 0.1206 - acc: 0.9284 - val_loss: 2.1962 - val_acc: 0.4118\n",
            "Epoch 75/100\n",
            "86/86 - 2s - loss: 0.1178 - acc: 0.9288 - val_loss: 2.3918 - val_acc: 0.4085\n",
            "Epoch 76/100\n",
            "86/86 - 2s - loss: 0.1164 - acc: 0.9295 - val_loss: 2.4139 - val_acc: 0.4118\n",
            "Epoch 77/100\n",
            "86/86 - 2s - loss: 0.1161 - acc: 0.9324 - val_loss: 2.3492 - val_acc: 0.4216\n",
            "Epoch 78/100\n",
            "86/86 - 2s - loss: 0.1039 - acc: 0.9371 - val_loss: 2.3533 - val_acc: 0.4150\n",
            "Epoch 79/100\n",
            "86/86 - 2s - loss: 0.1137 - acc: 0.9291 - val_loss: 2.2596 - val_acc: 0.4314\n",
            "Epoch 80/100\n",
            "86/86 - 2s - loss: 0.1173 - acc: 0.9299 - val_loss: 2.4025 - val_acc: 0.4052\n",
            "Epoch 81/100\n",
            "86/86 - 2s - loss: 0.1171 - acc: 0.9277 - val_loss: 2.3343 - val_acc: 0.4183\n",
            "Epoch 82/100\n",
            "86/86 - 2s - loss: 0.1112 - acc: 0.9320 - val_loss: 2.4193 - val_acc: 0.4379\n",
            "Epoch 83/100\n",
            "86/86 - 2s - loss: 0.1113 - acc: 0.9382 - val_loss: 2.3088 - val_acc: 0.4118\n",
            "Epoch 84/100\n",
            "86/86 - 2s - loss: 0.1162 - acc: 0.9251 - val_loss: 2.4093 - val_acc: 0.3856\n",
            "Epoch 85/100\n",
            "86/86 - 2s - loss: 0.1132 - acc: 0.9324 - val_loss: 2.3506 - val_acc: 0.3922\n",
            "Epoch 86/100\n",
            "86/86 - 2s - loss: 0.1181 - acc: 0.9346 - val_loss: 2.4257 - val_acc: 0.4085\n",
            "Epoch 87/100\n",
            "86/86 - 2s - loss: 0.1091 - acc: 0.9281 - val_loss: 2.5079 - val_acc: 0.3595\n",
            "Epoch 88/100\n",
            "86/86 - 2s - loss: 0.1051 - acc: 0.9331 - val_loss: 2.4502 - val_acc: 0.4020\n",
            "Epoch 89/100\n",
            "86/86 - 2s - loss: 0.1018 - acc: 0.9390 - val_loss: 2.3871 - val_acc: 0.4248\n",
            "Epoch 90/100\n",
            "86/86 - 2s - loss: 0.1048 - acc: 0.9350 - val_loss: 2.5439 - val_acc: 0.3693\n",
            "Epoch 91/100\n",
            "86/86 - 2s - loss: 0.1049 - acc: 0.9364 - val_loss: 2.3418 - val_acc: 0.4183\n",
            "Epoch 92/100\n",
            "86/86 - 2s - loss: 0.0999 - acc: 0.9404 - val_loss: 2.4082 - val_acc: 0.4020\n",
            "Epoch 93/100\n",
            "86/86 - 2s - loss: 0.0996 - acc: 0.9353 - val_loss: 2.3778 - val_acc: 0.4052\n",
            "Epoch 94/100\n",
            "86/86 - 2s - loss: 0.1033 - acc: 0.9306 - val_loss: 2.3940 - val_acc: 0.3954\n",
            "Epoch 95/100\n",
            "86/86 - 2s - loss: 0.1097 - acc: 0.9302 - val_loss: 2.3770 - val_acc: 0.4379\n",
            "Epoch 96/100\n",
            "86/86 - 2s - loss: 0.1057 - acc: 0.9364 - val_loss: 2.4369 - val_acc: 0.4281\n",
            "Epoch 97/100\n",
            "86/86 - 2s - loss: 0.1025 - acc: 0.9339 - val_loss: 2.1897 - val_acc: 0.4444\n",
            "Epoch 98/100\n",
            "86/86 - 2s - loss: 0.1080 - acc: 0.9371 - val_loss: 2.2094 - val_acc: 0.4281\n",
            "Epoch 99/100\n",
            "86/86 - 2s - loss: 0.1092 - acc: 0.9324 - val_loss: 2.5217 - val_acc: 0.4052\n",
            "Epoch 100/100\n",
            "86/86 - 2s - loss: 0.1185 - acc: 0.9270 - val_loss: 2.4816 - val_acc: 0.3856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe088c90e10>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSWtRWqurGZb"
      },
      "source": [
        "A slight improvement in the model performance is observed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdbeRmewq2rT"
      },
      "source": [
        "### label smoothing\n",
        "Now, as the model accuracy on training set is relatively high, it can also mean that the model is becoming overconfident. One strategy to deal with such situations is label smoothing, where instead of pushing the model to predict '1' as the truth value, we make it predict a sightly lower value like 0.9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YgFRVIkqzg_",
        "outputId": "fd89ba49-1f86-4aba-a51d-fc4b47b8852d"
      },
      "source": [
        "import tensorflow as tf\n",
        "# build a model with GloVe embeddings (trainable = True)\n",
        "glove_model_4 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dimension, weights=[embedding_matrix], input_length=max_len, trainable=True),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(256),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dropout(.25),\n",
        "    Dense(len(speakers\n",
        "    ), activation=\"softmax\")\n",
        "])\n",
        "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.1)\n",
        "optm = tf.keras.optimizers.Adam()\n",
        "glove_model_4.compile(optimizer=optm, loss=cce, metrics=[\"acc\"], loss_weights=loss_weights)\n",
        "glove_model_4.fit(X_train, Y_train, validation_split=.1, epochs=100, verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "86/86 - 8s - loss: 1.6279 - acc: 0.1461 - val_loss: 1.5796 - val_acc: 0.1209\n",
            "Epoch 2/100\n",
            "86/86 - 3s - loss: 1.4280 - acc: 0.2315 - val_loss: 1.5189 - val_acc: 0.2026\n",
            "Epoch 3/100\n",
            "86/86 - 3s - loss: 1.3154 - acc: 0.3060 - val_loss: 1.5494 - val_acc: 0.1471\n",
            "Epoch 4/100\n",
            "86/86 - 3s - loss: 1.1862 - acc: 0.4095 - val_loss: 1.5064 - val_acc: 0.1732\n",
            "Epoch 5/100\n",
            "86/86 - 3s - loss: 1.0571 - acc: 0.5011 - val_loss: 1.3562 - val_acc: 0.2614\n",
            "Epoch 6/100\n",
            "86/86 - 3s - loss: 0.9313 - acc: 0.6094 - val_loss: 1.2623 - val_acc: 0.3660\n",
            "Epoch 7/100\n",
            "86/86 - 3s - loss: 0.8489 - acc: 0.6679 - val_loss: 1.2600 - val_acc: 0.3889\n",
            "Epoch 8/100\n",
            "86/86 - 2s - loss: 0.7565 - acc: 0.7402 - val_loss: 1.2247 - val_acc: 0.4150\n",
            "Epoch 9/100\n",
            "86/86 - 3s - loss: 0.6942 - acc: 0.7976 - val_loss: 1.2651 - val_acc: 0.3987\n",
            "Epoch 10/100\n",
            "86/86 - 3s - loss: 0.6601 - acc: 0.8092 - val_loss: 1.2468 - val_acc: 0.4118\n",
            "Epoch 11/100\n",
            "86/86 - 3s - loss: 0.6295 - acc: 0.8376 - val_loss: 1.2710 - val_acc: 0.3856\n",
            "Epoch 12/100\n",
            "86/86 - 3s - loss: 0.5963 - acc: 0.8557 - val_loss: 1.3274 - val_acc: 0.3595\n",
            "Epoch 13/100\n",
            "86/86 - 3s - loss: 0.5839 - acc: 0.8681 - val_loss: 1.4059 - val_acc: 0.3562\n",
            "Epoch 14/100\n",
            "86/86 - 3s - loss: 0.5616 - acc: 0.8794 - val_loss: 1.3727 - val_acc: 0.3529\n",
            "Epoch 15/100\n",
            "86/86 - 3s - loss: 0.5479 - acc: 0.8914 - val_loss: 1.2793 - val_acc: 0.4020\n",
            "Epoch 16/100\n",
            "86/86 - 3s - loss: 0.5344 - acc: 0.8968 - val_loss: 1.2756 - val_acc: 0.4412\n",
            "Epoch 17/100\n",
            "86/86 - 3s - loss: 0.5288 - acc: 0.8983 - val_loss: 1.2963 - val_acc: 0.4052\n",
            "Epoch 18/100\n",
            "86/86 - 3s - loss: 0.5234 - acc: 0.9026 - val_loss: 1.3050 - val_acc: 0.3824\n",
            "Epoch 19/100\n",
            "86/86 - 3s - loss: 0.5225 - acc: 0.8990 - val_loss: 1.3405 - val_acc: 0.3856\n",
            "Epoch 20/100\n",
            "86/86 - 2s - loss: 0.5128 - acc: 0.9048 - val_loss: 1.3714 - val_acc: 0.3627\n",
            "Epoch 21/100\n",
            "86/86 - 3s - loss: 0.5040 - acc: 0.9095 - val_loss: 1.3155 - val_acc: 0.4020\n",
            "Epoch 22/100\n",
            "86/86 - 3s - loss: 0.5069 - acc: 0.9099 - val_loss: 1.4483 - val_acc: 0.3039\n",
            "Epoch 23/100\n",
            "86/86 - 3s - loss: 0.5005 - acc: 0.9095 - val_loss: 1.3182 - val_acc: 0.4150\n",
            "Epoch 24/100\n",
            "86/86 - 3s - loss: 0.4930 - acc: 0.9150 - val_loss: 1.3025 - val_acc: 0.4248\n",
            "Epoch 25/100\n",
            "86/86 - 3s - loss: 0.4825 - acc: 0.9204 - val_loss: 1.2777 - val_acc: 0.4608\n",
            "Epoch 26/100\n",
            "86/86 - 3s - loss: 0.4746 - acc: 0.9226 - val_loss: 1.3415 - val_acc: 0.4052\n",
            "Epoch 27/100\n",
            "86/86 - 3s - loss: 0.4743 - acc: 0.9270 - val_loss: 1.2896 - val_acc: 0.4510\n",
            "Epoch 28/100\n",
            "86/86 - 3s - loss: 0.4732 - acc: 0.9255 - val_loss: 1.3289 - val_acc: 0.4248\n",
            "Epoch 29/100\n",
            "86/86 - 3s - loss: 0.4740 - acc: 0.9262 - val_loss: 1.3316 - val_acc: 0.4248\n",
            "Epoch 30/100\n",
            "86/86 - 3s - loss: 0.4663 - acc: 0.9273 - val_loss: 1.3004 - val_acc: 0.4641\n",
            "Epoch 31/100\n",
            "86/86 - 3s - loss: 0.4657 - acc: 0.9284 - val_loss: 1.3161 - val_acc: 0.4477\n",
            "Epoch 32/100\n",
            "86/86 - 3s - loss: 0.4633 - acc: 0.9270 - val_loss: 1.3122 - val_acc: 0.4477\n",
            "Epoch 33/100\n",
            "86/86 - 3s - loss: 0.4590 - acc: 0.9299 - val_loss: 1.3207 - val_acc: 0.4608\n",
            "Epoch 34/100\n",
            "86/86 - 3s - loss: 0.4615 - acc: 0.9331 - val_loss: 1.3096 - val_acc: 0.4542\n",
            "Epoch 35/100\n",
            "86/86 - 3s - loss: 0.4551 - acc: 0.9335 - val_loss: 1.3457 - val_acc: 0.4444\n",
            "Epoch 36/100\n",
            "86/86 - 3s - loss: 0.4500 - acc: 0.9328 - val_loss: 1.3352 - val_acc: 0.4542\n",
            "Epoch 37/100\n",
            "86/86 - 3s - loss: 0.4523 - acc: 0.9360 - val_loss: 1.3231 - val_acc: 0.4444\n",
            "Epoch 38/100\n",
            "86/86 - 3s - loss: 0.4505 - acc: 0.9342 - val_loss: 1.3385 - val_acc: 0.4444\n",
            "Epoch 39/100\n",
            "86/86 - 3s - loss: 0.4687 - acc: 0.9241 - val_loss: 1.3803 - val_acc: 0.4248\n",
            "Epoch 40/100\n",
            "86/86 - 3s - loss: 0.4684 - acc: 0.9251 - val_loss: 1.3157 - val_acc: 0.4575\n",
            "Epoch 41/100\n",
            "86/86 - 3s - loss: 0.4519 - acc: 0.9320 - val_loss: 1.3730 - val_acc: 0.4412\n",
            "Epoch 42/100\n",
            "86/86 - 3s - loss: 0.4477 - acc: 0.9360 - val_loss: 1.3606 - val_acc: 0.4444\n",
            "Epoch 43/100\n",
            "86/86 - 3s - loss: 0.4516 - acc: 0.9313 - val_loss: 1.3780 - val_acc: 0.4281\n",
            "Epoch 44/100\n",
            "86/86 - 3s - loss: 0.4469 - acc: 0.9335 - val_loss: 1.3737 - val_acc: 0.4477\n",
            "Epoch 45/100\n",
            "86/86 - 3s - loss: 0.4413 - acc: 0.9371 - val_loss: 1.3325 - val_acc: 0.4444\n",
            "Epoch 46/100\n",
            "86/86 - 3s - loss: 0.4454 - acc: 0.9350 - val_loss: 1.3459 - val_acc: 0.4477\n",
            "Epoch 47/100\n",
            "86/86 - 3s - loss: 0.4446 - acc: 0.9331 - val_loss: 1.3519 - val_acc: 0.4542\n",
            "Epoch 48/100\n",
            "86/86 - 3s - loss: 0.4429 - acc: 0.9382 - val_loss: 1.3198 - val_acc: 0.4739\n",
            "Epoch 49/100\n",
            "86/86 - 3s - loss: 0.4376 - acc: 0.9404 - val_loss: 1.3205 - val_acc: 0.4608\n",
            "Epoch 50/100\n",
            "86/86 - 3s - loss: 0.4368 - acc: 0.9353 - val_loss: 1.3706 - val_acc: 0.4379\n",
            "Epoch 51/100\n",
            "86/86 - 2s - loss: 0.4419 - acc: 0.9331 - val_loss: 1.3535 - val_acc: 0.4575\n",
            "Epoch 52/100\n",
            "86/86 - 2s - loss: 0.4362 - acc: 0.9364 - val_loss: 1.3368 - val_acc: 0.4575\n",
            "Epoch 53/100\n",
            "86/86 - 2s - loss: 0.4384 - acc: 0.9342 - val_loss: 1.3298 - val_acc: 0.4412\n",
            "Epoch 54/100\n",
            "86/86 - 3s - loss: 0.4351 - acc: 0.9346 - val_loss: 1.3681 - val_acc: 0.4510\n",
            "Epoch 55/100\n",
            "86/86 - 3s - loss: 0.4425 - acc: 0.9339 - val_loss: 1.5326 - val_acc: 0.3824\n",
            "Epoch 56/100\n",
            "86/86 - 3s - loss: 0.4672 - acc: 0.9190 - val_loss: 1.4646 - val_acc: 0.3987\n",
            "Epoch 57/100\n",
            "86/86 - 3s - loss: 0.4634 - acc: 0.9233 - val_loss: 1.3792 - val_acc: 0.4248\n",
            "Epoch 58/100\n",
            "86/86 - 3s - loss: 0.4446 - acc: 0.9331 - val_loss: 1.3439 - val_acc: 0.4510\n",
            "Epoch 59/100\n",
            "86/86 - 3s - loss: 0.4378 - acc: 0.9375 - val_loss: 1.3467 - val_acc: 0.4379\n",
            "Epoch 60/100\n",
            "86/86 - 2s - loss: 0.4348 - acc: 0.9357 - val_loss: 1.3786 - val_acc: 0.4346\n",
            "Epoch 61/100\n",
            "86/86 - 2s - loss: 0.4389 - acc: 0.9368 - val_loss: 1.3721 - val_acc: 0.4281\n",
            "Epoch 62/100\n",
            "86/86 - 3s - loss: 0.4344 - acc: 0.9397 - val_loss: 1.3611 - val_acc: 0.4346\n",
            "Epoch 63/100\n",
            "86/86 - 3s - loss: 0.4340 - acc: 0.9360 - val_loss: 1.3661 - val_acc: 0.4346\n",
            "Epoch 64/100\n",
            "86/86 - 3s - loss: 0.4301 - acc: 0.9433 - val_loss: 1.3784 - val_acc: 0.4281\n",
            "Epoch 65/100\n",
            "86/86 - 2s - loss: 0.4330 - acc: 0.9390 - val_loss: 1.4127 - val_acc: 0.4281\n",
            "Epoch 66/100\n",
            "86/86 - 3s - loss: 0.4295 - acc: 0.9390 - val_loss: 1.3717 - val_acc: 0.4575\n",
            "Epoch 67/100\n",
            "86/86 - 3s - loss: 0.4296 - acc: 0.9411 - val_loss: 1.3639 - val_acc: 0.4477\n",
            "Epoch 68/100\n",
            "86/86 - 3s - loss: 0.4291 - acc: 0.9382 - val_loss: 1.3625 - val_acc: 0.4575\n",
            "Epoch 69/100\n",
            "86/86 - 3s - loss: 0.4307 - acc: 0.9390 - val_loss: 1.3824 - val_acc: 0.4510\n",
            "Epoch 70/100\n",
            "86/86 - 3s - loss: 0.4314 - acc: 0.9353 - val_loss: 1.3855 - val_acc: 0.4477\n",
            "Epoch 71/100\n",
            "86/86 - 3s - loss: 0.4339 - acc: 0.9371 - val_loss: 1.4169 - val_acc: 0.4118\n",
            "Epoch 72/100\n",
            "86/86 - 3s - loss: 0.4328 - acc: 0.9342 - val_loss: 1.3819 - val_acc: 0.4444\n",
            "Epoch 73/100\n",
            "86/86 - 3s - loss: 0.4300 - acc: 0.9393 - val_loss: 1.4730 - val_acc: 0.4020\n",
            "Epoch 74/100\n",
            "86/86 - 3s - loss: 0.4350 - acc: 0.9375 - val_loss: 1.3715 - val_acc: 0.4379\n",
            "Epoch 75/100\n",
            "86/86 - 3s - loss: 0.4362 - acc: 0.9346 - val_loss: 1.4842 - val_acc: 0.4085\n",
            "Epoch 76/100\n",
            "86/86 - 3s - loss: 0.4453 - acc: 0.9317 - val_loss: 1.3790 - val_acc: 0.4575\n",
            "Epoch 77/100\n",
            "86/86 - 3s - loss: 0.4341 - acc: 0.9368 - val_loss: 1.3614 - val_acc: 0.4346\n",
            "Epoch 78/100\n",
            "86/86 - 3s - loss: 0.4312 - acc: 0.9375 - val_loss: 1.3934 - val_acc: 0.4281\n",
            "Epoch 79/100\n",
            "86/86 - 3s - loss: 0.4291 - acc: 0.9360 - val_loss: 1.3689 - val_acc: 0.4608\n",
            "Epoch 80/100\n",
            "86/86 - 3s - loss: 0.4285 - acc: 0.9400 - val_loss: 1.3696 - val_acc: 0.4346\n",
            "Epoch 81/100\n",
            "86/86 - 3s - loss: 0.4278 - acc: 0.9415 - val_loss: 1.3690 - val_acc: 0.4477\n",
            "Epoch 82/100\n",
            "86/86 - 3s - loss: 0.4284 - acc: 0.9371 - val_loss: 1.3817 - val_acc: 0.4412\n",
            "Epoch 83/100\n",
            "86/86 - 3s - loss: 0.4274 - acc: 0.9379 - val_loss: 1.3764 - val_acc: 0.4444\n",
            "Epoch 84/100\n",
            "86/86 - 3s - loss: 0.4303 - acc: 0.9364 - val_loss: 1.3602 - val_acc: 0.4575\n",
            "Epoch 85/100\n",
            "86/86 - 2s - loss: 0.4278 - acc: 0.9393 - val_loss: 1.3665 - val_acc: 0.4412\n",
            "Epoch 86/100\n",
            "86/86 - 2s - loss: 0.4309 - acc: 0.9400 - val_loss: 1.4040 - val_acc: 0.4379\n",
            "Epoch 87/100\n",
            "86/86 - 3s - loss: 0.4303 - acc: 0.9390 - val_loss: 1.3557 - val_acc: 0.4542\n",
            "Epoch 88/100\n",
            "86/86 - 2s - loss: 0.4273 - acc: 0.9382 - val_loss: 1.3803 - val_acc: 0.4575\n",
            "Epoch 89/100\n",
            "86/86 - 2s - loss: 0.4290 - acc: 0.9386 - val_loss: 1.3617 - val_acc: 0.4477\n",
            "Epoch 90/100\n",
            "86/86 - 3s - loss: 0.4237 - acc: 0.9408 - val_loss: 1.3694 - val_acc: 0.4477\n",
            "Epoch 91/100\n",
            "86/86 - 3s - loss: 0.4273 - acc: 0.9375 - val_loss: 1.3805 - val_acc: 0.4510\n",
            "Epoch 92/100\n",
            "86/86 - 2s - loss: 0.4245 - acc: 0.9397 - val_loss: 1.3845 - val_acc: 0.4412\n",
            "Epoch 93/100\n",
            "86/86 - 2s - loss: 0.4307 - acc: 0.9368 - val_loss: 1.3991 - val_acc: 0.4477\n",
            "Epoch 94/100\n",
            "86/86 - 3s - loss: 0.4312 - acc: 0.9379 - val_loss: 1.3855 - val_acc: 0.4510\n",
            "Epoch 95/100\n",
            "86/86 - 3s - loss: 0.4287 - acc: 0.9411 - val_loss: 1.4350 - val_acc: 0.4052\n",
            "Epoch 96/100\n",
            "86/86 - 3s - loss: 0.4276 - acc: 0.9386 - val_loss: 1.4446 - val_acc: 0.4150\n",
            "Epoch 97/100\n",
            "86/86 - 3s - loss: 0.4311 - acc: 0.9350 - val_loss: 1.4993 - val_acc: 0.4085\n",
            "Epoch 98/100\n",
            "86/86 - 2s - loss: 0.4387 - acc: 0.9335 - val_loss: 1.4176 - val_acc: 0.4248\n",
            "Epoch 99/100\n",
            "86/86 - 2s - loss: 0.4396 - acc: 0.9310 - val_loss: 1.5190 - val_acc: 0.4052\n",
            "Epoch 100/100\n",
            "86/86 - 2s - loss: 0.4424 - acc: 0.9317 - val_loss: 1.4760 - val_acc: 0.4150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe075f64cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    }
  ]
}